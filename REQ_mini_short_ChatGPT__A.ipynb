{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Data saved to output.xlsx\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 70\u001b[0m\n\u001b[1;32m     66\u001b[0m save_to_excel(entries, output_file)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing complete. Data saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-t5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentenceTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract text from a .docx file and find the word \"latency\"\n",
    "def process_docx(file_path):\n",
    "    document = Document(file_path)\n",
    "    found_entries = []\n",
    "    chapter_text = []\n",
    "    chapter_start = False\n",
    "    chapter = \"\"\n",
    "    \n",
    "    for para in document.paragraphs:\n",
    "        if para.style.name.startswith('Heading'):  # Identify chapter start by heading styles\n",
    "            if chapter_start:  # Save previous chapter if it had \"latency\"\n",
    "                if 'latency' in \" \".join(chapter_text).lower():\n",
    "                    chapter_text_limited = \" \".join(\" \".join(chapter_text).split()[:5000])\n",
    "                    found_entries.append({\n",
    "                        'file_name': os.path.basename(file_path),\n",
    "                        'chapter': chapter,\n",
    "                        'chapter_text': chapter_text_limited\n",
    "                    })\n",
    "            chapter_start = True\n",
    "            chapter = para.text\n",
    "            chapter_text = [para.text]\n",
    "        elif chapter_start:\n",
    "            chapter_text.append(para.text)\n",
    "\n",
    "    # Check last chapter\n",
    "    if chapter_start and 'latency' in \" \".join(chapter_text).lower():\n",
    "        chapter_text_limited = \" \".join(\" \".join(chapter_text).split()[:5000])\n",
    "        found_entries.append({\n",
    "            'file_name': os.path.basename(file_path),\n",
    "            'chapter': chapter,\n",
    "            'chapter_text': chapter_text_limited\n",
    "        })\n",
    "\n",
    "    return found_entries\n",
    "\n",
    "# Main function to process all .docx files in the folder\n",
    "def process_folder(folder_path):\n",
    "    all_entries = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.docx'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            entries = process_docx(file_path)\n",
    "            all_entries.extend(entries)\n",
    "\n",
    "    return all_entries\n",
    "\n",
    "# Save the extracted information to an Excel file\n",
    "def save_to_excel(entries, output_file):\n",
    "    df = pd.DataFrame(entries)\n",
    "    if not df.empty:\n",
    "        df.to_excel(output_file, index=False)\n",
    "    else:\n",
    "        print(\"No entries found.\")\n",
    "\n",
    "# Specify the folder containing the .docx files and the output Excel file\n",
    "folder_path = \"./short__2\"\n",
    "output_file = 'output.xlsx'\n",
    "\n",
    "# Process the folder and save the results to an Excel file\n",
    "entries = process_folder(folder_path)\n",
    "save_to_excel(entries, output_file)\n",
    "\n",
    "print(f\"Processing complete. Data saved to {output_file}\")\n",
    "\n",
    "model = SentenceTransformer(\"sentence-t5-large\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: create requirements based on each of these sections\n",
    "\n",
    "In the last step, we create new requirements based on the sections identified in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m dfDistances \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)       \u001b[38;5;66;03m#(\"./temp_sections_for_generation.xlsx\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m lstDist \u001b[38;5;241m=\u001b[39m dfDistances\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-t5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentenceTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "# read the lstDst from the Excel file\n",
    "dfDistances = pd.read_excel(\"./output.xlsx\")       #(\"./temp_sections_for_generation.xlsx\")\n",
    "\n",
    "lstDist = dfDistances.values.tolist()\n",
    "\n",
    "model = SentenceTransformer(\"sentence-t5-large\")\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#modelP = \"microsoft/Phi-3-mini-128k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Set the manual seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Load the model and tokenizer\n",
    "modelInstr = AutoModelForCausalLM.from_pretrained(\n",
    "    modelP, \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=torch.float16, \n",
    "    revision=\"float16\", \n",
    "    attn_implementation='eager',\n",
    ")\n",
    "\n",
    "tokenizerInstr = AutoTokenizer.from_pretrained(modelP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0)\n",
    "\n",
    "modelInstr = AutoModelForCausalLM.from_pretrained(\n",
    "    modelP, \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    "    attn_implementation='eager',\n",
    ")\n",
    "tokenizerInstr = AutoTokenizer.from_pretrained(modelP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRequirement(content, type, model, tokenizer):\n",
    "    content1 = content.split(\",\")\n",
    "    content1 = [x for x in content1[1:] if x not in ['', \" ''\", \" '']\"]]\n",
    "    content_str = \" \".join(content1)\n",
    "\n",
    "    # this is about signalling, payload, c/c. \n",
    "    # typeStr = type.split(\"_\")[1]\n",
    "    \n",
    "    # strContent = f\"Based on this : {content_str}. Write the requirement in the following format 'The {typeStr} of the system shall ' \"\n",
    "    strContent = f\"Based on this : {content_str}. Write the requirement in the following format 'The system shall ' \"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": strContent},\n",
    "    ]\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "\n",
    "    output = pipe(messages, **generation_args)\n",
    "    \n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRequirement2(content, section, document, model, tokenizer):\n",
    "    content1 = content.split(\",\")\n",
    "    content1 = [x for x in content1[1:] if x not in ['', \" ''\", \" '']\"]]\n",
    "    content_str = \" \".join(content1)\n",
    "\n",
    "    # check if the text contains the word Figure\n",
    "    # if it does, then we add a footnote before to warn the user\n",
    "    if \"Figure\" in content_str:\n",
    "        #strContent = f\"Based on this : {content_str}. Write the requirement in the following format 'The system shall '. Start with '* This part of the standard contains a figure, the generated requirement can be inaccurate, please consult the original text for details.' \"\n",
    "        strContent = f\"Based on this : {content_str}, write the requirement about {section} from {document}. Add this text at the beginning: '* This part of the standard contains a figure, the generated requirement can be inaccurate, please consult the original text for details.' \"\n",
    "    # the same for tables, at least the ones that we can identify\n",
    "    elif \"Table \" in content_str[:10]:\n",
    "        #strContent = f\"Summarize this table. {content_str}. Based on this summary, write the requirement in the following format 'The system shall '. Start with '* This part of the standard contains a table, the generated requirement can be inaccurate, please consult the original text for details.' \"\n",
    "        strContent = f\"Summarize this table. {content_str}. Based on this summary, write a requirement about {section} from {document}. Add this text at the beginning '* This part of the standard contains a table, the generated requirement can be inaccurate, please consult the original text for details.' \"\n",
    "    \n",
    "    # and for the empty text, e.g., when the word latency is only in the title\n",
    "    # we do not generate anything and warn the user\n",
    "    elif len(content_str) < 2:\n",
    "        return \"This section is empty. The word latency is probably only in a section title\"\n",
    "    # otherwise, we generate the requirement\n",
    "    else: \n",
    "        #strContent = f\"Based on this : {content_str}. Write the requirement in the following format 'The system shall ' \"\n",
    "        strContent = f\"Write a requirement about {section} from {document} based on this: {content_str}. \"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": strContent},\n",
    "    ]\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0,\n",
    "    }\n",
    "\n",
    "    output = pipe(messages, **generation_args)\n",
    "    \n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstGenerated = []\n",
    "iCounter = 0\n",
    "\n",
    "# we generate new requirements for the partially covered ones\n",
    "if len(lstDist) > 0:\n",
    "    for oneLine in tqdm(lstDist):\n",
    "        if (len(oneLine[3]) < 4095):\n",
    "            strRequirement = createRequirement2(oneLine[3], oneLine[0], oneLine[1], modelInstr, tokenizerInstr)\n",
    "            lstGenerated.append([oneLine[0], oneLine[1], oneLine[2], oneLine[3], strRequirement])\n",
    "            dfOutput = pd.DataFrame(lstGenerated, columns=[\"Section\", \"Document\", \"Distance\", \"Content\", \"Generated requirement\"])\n",
    "            dfOutput.to_excel(generatedFile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOutput = pd.DataFrame(lstGenerated, columns=[\"Section\", \"Document\",  \"Distance\", \"Content\", \"Generated requirement\"])\n",
    "dfOutput.to_excel(generatedFile, index=False)\n",
    "\n",
    "# and generate html\n",
    "dfOutput.to_html(\"generated.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
