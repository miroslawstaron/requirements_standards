{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3REQ system\n",
    "\n",
    "Requirement analysis system.\n",
    "\n",
    "The flow is presented in the following figure: \n",
    "\n",
    "<img src=\"flow.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "Summary:\n",
    "1. Extract sections that contain words like \"latency\"\n",
    "2. Find if they resemble requirements or not\n",
    "3. Check if they are specific kinds of requirements (like signalling)\n",
    "4. Write new requirements based on the text in these sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wilhelmmeding/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "# docx2python is used to extract text, images, tables, and other data from .docx files\n",
    "from docx2python import docx2python\n",
    "\n",
    "# os module provides functions for interacting with the operating system\n",
    "import os\n",
    "\n",
    "# numpy is used for mathematical operations on large, multi-dimensional arrays and matrices\n",
    "import numpy as np\n",
    "\n",
    "# pandas is used for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# TSNE from sklearn.manifold is used for dimensionality reduction\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# matplotlib.pyplot is used for creating static, animated, and interactive visualizations in Python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SentenceTransformer is used for training and using transformer models for generating sentence embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# tqdm is used to make loops show a smart progress meter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# torch is the main package in PyTorch, it provides a multi-dimensional array with support for autograd operations like backward()\n",
    "import torch\n",
    "\n",
    "# AutoModelForCausalLM, AutoTokenizer, pipeline are from the transformers library by Hugging Face which provides state-of-the-art machine learning models like BERT, GPT-2, etc.\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# euclidean distance and cosine distance\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# random generator for the last figure\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input folder\n",
    "docInputFolder = \"./23_standards\"\n",
    "\n",
    "# name of file with generated requirements\n",
    "generatedFile = \"./output_generated_mini.xlsx\"\n",
    "\n",
    "# embedding model\n",
    "# in order of size -- the last one is only for A6000 :) \n",
    "\n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model = SentenceTransformer(\"sentence-t5-large\")\n",
    "# model = SentenceTransformer(\"sentence-transformers/gtr-t5-xxl\")\n",
    "\n",
    "\n",
    "# generative model\n",
    "modelP = \"microsoft/Phi-3-mini-128k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: List the documents's sections with \"latency\"\n",
    "\n",
    "In the first step, we go through the documents in the folder \"input_standards\" and we extract which sections of these documents contain th word \"latency\". We store the results in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLatencySections(doc):\n",
    "    strSectionTitle = \"\"\n",
    "    dictSections = {}\n",
    "    listLatency = []\n",
    "    skippedSections = 0\n",
    "\n",
    "    doc_result = docx2python(doc,paragraph_styles = True, html=True)\n",
    "\n",
    "    # we iterate over all lines\n",
    "    # look for the section titles (which have the tag <h1>, <h2>, <h3>, etc.)\n",
    "    # then we add the content of each section to the dictionary\n",
    "    # and if there is a word \"latency\" somewhere in the section, we add the section title to the listLatency\n",
    "    for oneLine in tqdm(doc_result.text.split('\\n')):\n",
    "        if \"<h\" in oneLine:\n",
    "            strSectionTitle = oneLine\n",
    "            dictSections[strSectionTitle] = []\n",
    "\n",
    "        if strSectionTitle != \"\":  \n",
    "            dictSections[strSectionTitle].append(oneLine)\n",
    "\n",
    "        keywordsInLine = [\"latency\"]\n",
    "        keywordsInSections = [\"references\", \n",
    "                              \"introduction\", \n",
    "                              \"definition\", \n",
    "                              \"abstract\", \n",
    "                              \"conclusion\", \n",
    "                              \"acknowledgements\", \n",
    "                              \"appendix\", \n",
    "                              \"table of contents\", \n",
    "                              \"table of figures\", \n",
    "                              \"table of tables\",  \n",
    "                              \"bibliography\", \n",
    "                              \"index\", \n",
    "                              \"glossary\", \n",
    "                              \"list of figures\", \n",
    "                              \"list of tables\", \n",
    "                              \"list of abbreviations\", \n",
    "                              \"list of symbols\", \n",
    "                              \"list of terms\", \n",
    "                              \"list of equations\", \n",
    "                              \"list of algorithms\", \n",
    "                              \"list of acronyms\", \n",
    "                              \"list of illustrations\", \n",
    "                              \"list of appendices\"]\n",
    "\n",
    "        if any(word in oneLine.lower() for word in keywordsInLine) and not any(word in strSectionTitle.lower() for word in keywordsInSections): \n",
    "            listLatency.append(strSectionTitle)\n",
    "        else:\n",
    "            skippedSections += 1\n",
    "            \n",
    "            \n",
    "    # remove the keys from the dictionary if they are not part of the listLatency\n",
    "    # as we want to get only the relevant sections, i.e., the one with the word latency\n",
    "    for key in list(dictSections.keys()):\n",
    "        if key not in listLatency:\n",
    "            del dictSections[key]\n",
    "\n",
    "    # print(\"Skipped sections: \", skippedSections)\n",
    "\n",
    "    # return the dictionary with the relevant sections\n",
    "    return dictSections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the return list of all the lines in the document\n",
    "lstAllLines = []\n",
    "\n",
    "# for each .docx file in the input folder\n",
    "# extract the sections with latency using the extractLatencySections function\n",
    "# and print the sections\n",
    "for doc in tqdm(os.listdir(docInputFolder)):    \n",
    "\n",
    "    if doc.endswith(\".docx\"):\n",
    "        \n",
    "        # since things can go wrong with the latency library, \n",
    "        # we use a try except block to avoid the program to stop\n",
    "        try: \n",
    "            dictSections = extractLatencySections(os.path.join(docInputFolder, doc))\n",
    "        \n",
    "            # we list the content\n",
    "            # as a long list of sections \n",
    "            for key in dictSections:\n",
    "\n",
    "                lstOneLine = [key, doc]\n",
    "\n",
    "                for line in dictSections[key]:\n",
    "                    lstOneLine.append(line)\n",
    "                    \n",
    "                lstAllLines.append(lstOneLine)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {doc}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = pd.DataFrame(lstAllLines)\n",
    "\n",
    "dfTest[1].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list with all embeddings for the sections\n",
    "lstEmbeddings = []\n",
    "iCounter = 0\n",
    "\n",
    "for oneLine in tqdm(lstAllLines):\n",
    "\n",
    "    # the content of the section starts on the third position of the list\n",
    "    sentences = oneLine[3:]\n",
    "\n",
    "    # Sentences are encoded by calling model.encode()\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Print the average embeddings for all the sentences \n",
    "    # in this section\n",
    "    avg_embeddings = embeddings.mean(axis=0)\n",
    "    \n",
    "    lstOneLine = [oneLine[0], oneLine[1], 2, str(sentences).replace(\"$\", \"_\").replace(\"\\n\", \"_\"), avg_embeddings]\n",
    "\n",
    "    lstEmbeddings.append(lstOneLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lstEmbeddings to an xlsx file topic_relevant.xlsx\n",
    "df = pd.DataFrame(lstEmbeddings, columns=[\"Section\", \"Document\", \"Category\", \"Sentences\", \"Embeddings\"])\n",
    "df.to_excel(\"topic_relevant.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# let's plot these average embeddings using t-SNE\n",
    "\n",
    "# we create a list with the embeddings\n",
    "lstEmbeddingsNP = np.array([x[4] for x in lstEmbeddings])\n",
    "\n",
    "# we use t-SNE to reduce the dimensionality of the embeddings\n",
    "tsne = TSNE(n_components=2, verbose=0, perplexity=3, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(lstEmbeddingsNP)\n",
    "\n",
    "# we plot the t-SNE results\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "plt.scatter(tsne_results[:,0], tsne_results[:,1])\n",
    "\n",
    "# Add labels to each dot\n",
    "for i, label in enumerate([x[0] for x in lstEmbeddings]):\n",
    "    plt.text(tsne_results[i, 0], tsne_results[i, 1], label[3:10])\n",
    "\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Requirements classes\n",
    "\n",
    "Checking whether these are signalling, payload, etc. requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the requirements from the excel file requirements.xlsx, worksheet LR\n",
    "df = pd.read_excel(\"req_classes.xlsx\", sheet_name=\"LR\")\n",
    "\n",
    "# convert to list\n",
    "lstRequirements = df.values.tolist()\n",
    "lstRequirements[0]\n",
    "\n",
    "# now we calculate the embeddings for each of these requirements\n",
    "lstEmbeddingsReq = []\n",
    "\n",
    "for oneLine in tqdm(lstRequirements):\n",
    "    \n",
    "        # the content of the section starts on the third position of the list\n",
    "        sentences = oneLine[1]\n",
    "    \n",
    "        # Sentences are encoded by calling model.encode()\n",
    "        embeddings = model.encode(sentences)\n",
    "        \n",
    "        # Print the average embeddings for all the sentences \n",
    "        # in this section\n",
    "        avg_embedding = embeddings\n",
    "        \n",
    "        lstOneLine = [oneLine[0], 'latency', oneLine[1], oneLine[1], avg_embedding]\n",
    "    \n",
    "        lstEmbeddingsReq.append(lstOneLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we calculate the euclidean distance between the requirements and the sections\n",
    "# that are relevant\n",
    "lstDist = []\n",
    "lstRelevantDist = []\n",
    "\n",
    "for oneLine in tqdm(lstEmbeddings):\n",
    "    for oneLineReq in lstEmbeddingsReq:\n",
    "        # euclidean distance between the two embeddings\n",
    "        dist = distance.cosine(oneLine[4], oneLineReq[4])\n",
    "        lstDist.append([oneLine[0], oneLine[1], oneLineReq[0], dist, oneLine[3]])\n",
    "\n",
    "# now we sort the list by the distance\n",
    "lstDist.sort(key=lambda x: x[2])\n",
    "\n",
    "# and we print them\n",
    "for i in range(len(lstDist)):\n",
    "    # print(f\"Section {lstDist[i][0]} is close to requirement {lstDist[i][2]} with distance {lstDist[i][3]:.2f}\")\n",
    "    # add this to a list\n",
    "    lstRelevantDist.append([lstDist[i][0], lstDist[i][1], lstDist[i][2], lstDist[i][3], lstDist[i][4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the list to an Excel file\n",
    "dfOutput = pd.DataFrame(lstRelevantDist, columns=[\"Section\", \"Document\", \"Requirement\", \"Distance\", \"Content\"])\n",
    "\n",
    "# sort it by section and document\n",
    "dfOutput = dfOutput.sort_values(by=[\"Section\", \"Document\"])\n",
    "\n",
    "# average the distance in dfOutput per section, document and requirement\n",
    "dfOutput[\"Distance\"] = dfOutput[\"Distance\"].astype(float)\n",
    "dfGrouped = dfOutput.groupby([\"Section\", \"Document\", \"Requirement\", \"Content\"])\n",
    "\n",
    "#convert dfGrouped to a dataframe\n",
    "dfGrouped = dfGrouped.agg({\"Distance\": \"mean\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check the minimum distance per section and document\n",
    "dfGrouped = dfGrouped.sort_values(by=[\"Section\", \"Document\", \"Distance\"])\n",
    "\n",
    "dfGrouped = dfGrouped.groupby([\"Section\", \"Document\"]).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we turn this into a list of lists\n",
    "lstRelevantDistGroup = dfGrouped.values.tolist()\n",
    "\n",
    "print(len(lstRelevantDistGroup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe and save to excel lstRelevantDist\n",
    "dfRelevantDist = pd.DataFrame(lstRelevantDistGroup, columns=[\"Section\", \"Document\", \"Requirement\", \"Content\", \"Distance\"])\n",
    "\n",
    "dfRelevantDist.to_excel(\"./classified_requirements.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Check if the requirements exist in the database\n",
    "\n",
    "In this step, we check if the text that we identified so far is covered by the requirements that exist in the database. We use the sentence transformers to get the embeddings of the text and then we compare them to the existin sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholdPartial = 0.15\n",
    "thresholdCovered = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the requirements from the Excel file\n",
    "dfRelevantSections = pd.read_excel(\"./classified_requirements.xlsx\")\n",
    "\n",
    "lstRelevantSections = dfRelevantSections.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstRelevantSections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentenceTransformer(\"sentence-t5-large\")\n",
    "# model = SentenceTransformer(\"sentence-transformers/gtr-t5-xxl\")\n",
    "\n",
    "# list with all embeddings for the sections\n",
    "lstRelevantEmbeddings = []\n",
    "iCounter = 0\n",
    "\n",
    "for oneLine in tqdm(lstRelevantSections):\n",
    "\n",
    "    # the content of the section starts on the third position of the list\n",
    "    sentences = oneLine[3]\n",
    "\n",
    "    # Sentences are encoded by calling model.encode()\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Print the average embeddings for all the sentences \n",
    "    # in this section\n",
    "    # avg_embedding = np.mean(embeddings, axis=0)\n",
    "    \n",
    "    lstOneLine = [oneLine[0], oneLine[1], 2, str(sentences).replace(\"$\", \"_\").replace(\"\\n\", \"_\"), embeddings]\n",
    "\n",
    "    lstRelevantEmbeddings.append(lstOneLine)\n",
    "\n",
    "    iCounter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, read the requirements from the requirements database, file 20_requirements.xlsx\n",
    "dfTRequirements = pd.read_excel(\"existing_requirements.xlsx\")\n",
    "\n",
    "# convert to list\n",
    "lstTRequirements = dfTRequirements.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstTRequirements[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the embeddings\n",
    "# model = SentenceTransformer(\"sentence-t5-large\")\n",
    "\n",
    "# list with all embeddings for the sections\n",
    "lstTRequirementsEmbeddings = []\n",
    "iCounter = 0\n",
    "\n",
    "for oneLine in tqdm(lstTRequirements):\n",
    "\n",
    "    # the content of the section starts on the requirement text\n",
    "    sentences = oneLine[1]\n",
    "\n",
    "    # Sentences are encoded by calling model.encode()\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Print the average embeddings for all the sentences \n",
    "    # in this section\n",
    "    #avg_embedding = np.mean(embeddings, axis=0)\n",
    "    \n",
    "    lstOneLine = [oneLine[0], oneLine[1], 2, str(sentences).replace(\"$\", \"_\").replace(\"\\n\", \"_\"), embeddings]\n",
    "\n",
    "    lstTRequirementsEmbeddings.append(lstOneLine)\n",
    "\n",
    "    iCounter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the embeddings for all the lstTRequirementsEmbeddings\n",
    "lstTRequirementsEmbeddingsNP = np.array([x[4] for x in lstTRequirementsEmbeddings])\n",
    "\n",
    "tRequirementsAvgEmbeddings = np.mean(lstTRequirementsEmbeddingsNP, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, calculate the average distance of all the relevant sections to the average requirements\n",
    "lstDist = []\n",
    "\n",
    "for oneLine in tqdm(lstRelevantEmbeddings):\n",
    "    # euclidean distance between the two embeddings\n",
    "    dist = distance.cosine(oneLine[4], tRequirementsAvgEmbeddings)\n",
    "    lstDist.append([oneLine[0], oneLine[1], dist, oneLine[3]])\n",
    "\n",
    "# now we sort the list by the distance\n",
    "lstDist.sort(key=lambda x: x[3])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bar plot for the distances lstDist[2]\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "# Create a bar plot\n",
    "plt.bar([x[0] for x in lstDist], [x[2] for x in lstDist])\n",
    "\n",
    "# Add labels to each dot\n",
    "for i, label in enumerate([x[1] for x in lstDist]):\n",
    "    plt.text(i, lstDist[i][2], label)\n",
    "\n",
    "# add horizontal lines for the thresholds\n",
    "plt.axhline(y=thresholdPartial, color='orange', linestyle='-')\n",
    "plt.axhline(y=thresholdCovered, color='red', linestyle='-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results using the scatter plot, where\n",
    "# the x-axis is the distance to the average requirement\n",
    "# the y-axis is always 1\n",
    "# the color is red for the requirements\n",
    "\n",
    "# Create a color map based on x[2]\n",
    "color_map = {0: 'blue', 1: 'green', 2: 'red'}\n",
    "colors = [color_map[0] for x in lstDist]\n",
    "\n",
    "# we plot the t-SNE results\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "plt.scatter([x[2] for x in lstDist], [random.random() * 2 for x in lstDist], c=colors, s=100, alpha=0.5)\n",
    "\n",
    "# Add labels to each dot\n",
    "#for i, label in enumerate([x[3] for x in lstDist]):\n",
    "#    plt.text([x[2] for x in lstDist][i], 1, label[:20]+'...', rotation='vertical')\n",
    "\n",
    "# Add vertical lines at 0.33 and 1.0\n",
    "plt.axvline(x=thresholdPartial, color='orange', linestyle='--')\n",
    "plt.axvline(x=thresholdCovered, color='red', linestyle='--')\n",
    "\n",
    "# Set the limits of the x-axis\n",
    "plt.xlim(0, 2)\n",
    "plt.ylim(0, 2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lstDst\n",
    "dfDistances = pd.DataFrame(lstDist, columns=[\"Section\", \"Document\", \"Distance\", \"Content\"])\n",
    "\n",
    "dfDistances.to_excel(\"./temp_sections_for_generation.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: create requirements based on each of these sections\n",
    "\n",
    "In the last step, we create new requirements based on the sections identified in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the lstDst from the Excel file\n",
    "dfDistances = pd.read_excel(\"./temp_sections_for_generation.xlsx\")\n",
    "\n",
    "lstDist = dfDistances.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0)\n",
    "\n",
    "modelInstr = AutoModelForCausalLM.from_pretrained(\n",
    "    modelP, \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    "    attn_implementation='eager',\n",
    ")\n",
    "tokenizerInstr = AutoTokenizer.from_pretrained(modelP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRequirement(content, type, model, tokenizer):\n",
    "    content1 = content.split(\",\")\n",
    "    content1 = [x for x in content1[1:] if x not in ['', \" ''\", \" '']\"]]\n",
    "    content_str = \" \".join(content1)\n",
    "\n",
    "    # this is about signalling, payload, c/c. \n",
    "    # typeStr = type.split(\"_\")[1]\n",
    "    \n",
    "    # strContent = f\"Based on this : {content_str}. Write the requirement in the following format 'The {typeStr} of the system shall ' \"\n",
    "    strContent = f\"Based on this : {content_str}. Write the requirement in the following format 'The system shall ' \"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": strContent},\n",
    "    ]\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "\n",
    "    output = pipe(messages, **generation_args)\n",
    "    \n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRequirement2(content, section, document, model, tokenizer):\n",
    "    content1 = content.split(\",\")\n",
    "    content1 = [x for x in content1[1:] if x not in ['', \" ''\", \" '']\"]]\n",
    "    content_str = \" \".join(content1)\n",
    "\n",
    "    # check if the text contains the word Figure\n",
    "    # if it does, then we add a footnote before to warn the user\n",
    "    if \"Figure\" in content_str:\n",
    "        #strContent = f\"Based on this : {content_str}. Write the requirement in the following format 'The system shall '. Start with '* This part of the standard contains a figure, the generated requirement can be inaccurate, please consult the original text for details.' \"\n",
    "        strContent = f\"Based on this : {content_str}, write the requirement about {section} from {document}. Add this text at the beginning: '* This part of the standard contains a figure, the generated requirement can be inaccurate, please consult the original text for details.' \"\n",
    "    # the same for tables, at least the ones that we can identify\n",
    "    elif \"Table \" in content_str[:10]:\n",
    "        #strContent = f\"Summarize this table. {content_str}. Based on this summary, write the requirement in the following format 'The system shall '. Start with '* This part of the standard contains a table, the generated requirement can be inaccurate, please consult the original text for details.' \"\n",
    "        strContent = f\"Summarize this table. {content_str}. Based on this summary, write a requirement about {section} from {document}. Add this text at the beginning '* This part of the standard contains a table, the generated requirement can be inaccurate, please consult the original text for details.' \"\n",
    "    \n",
    "    # and for the empty text, e.g., when the word latency is only in the title\n",
    "    # we do not generate anything and warn the user\n",
    "    elif len(content_str) < 2:\n",
    "        return \"This section is empty. The word latency is probably only in a section title\"\n",
    "    # otherwise, we generate the requirement\n",
    "    else: \n",
    "        #strContent = f\"Based on this : {content_str}. Write the requirement in the following format 'The system shall ' \"\n",
    "        strContent = f\"Write a requirement about {section} from {document} based on this: {content_str}. \"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": strContent},\n",
    "    ]\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0,\n",
    "    }\n",
    "\n",
    "    output = pipe(messages, **generation_args)\n",
    "    \n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstGenerated = []\n",
    "iCounter = 0\n",
    "\n",
    "# we generate new requirements for the partially covered ones\n",
    "if len(lstDist) > 0:\n",
    "    for oneLine in tqdm(lstDist):\n",
    "        if (len(oneLine[3]) < 4095):\n",
    "            strRequirement = createRequirement2(oneLine[3], oneLine[0], oneLine[1], modelInstr, tokenizerInstr)\n",
    "            lstGenerated.append([oneLine[0], oneLine[1], oneLine[2], oneLine[3], strRequirement])\n",
    "            dfOutput = pd.DataFrame(lstGenerated, columns=[\"Section\", \"Document\", \"Distance\", \"Content\", \"Generated requirement\"])\n",
    "            dfOutput.to_excel(generatedFile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOutput = pd.DataFrame(lstGenerated, columns=[\"Section\", \"Document\",  \"Distance\", \"Content\", \"Generated requirement\"])\n",
    "dfOutput.to_excel(generatedFile, index=False)\n",
    "\n",
    "# and generate html\n",
    "dfOutput.to_html(\"generated.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
