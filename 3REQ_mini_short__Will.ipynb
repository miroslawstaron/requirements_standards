{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "# docx2python is used to extract text, images, tables, and other data from .docx files\n",
    "from docx2python import docx2python\n",
    "\n",
    "# os module provides functions for interacting with the operating system\n",
    "import os\n",
    "\n",
    "# numpy is used for mathematical operations on large, multi-dimensional arrays and matrices\n",
    "import numpy as np\n",
    "\n",
    "# pandas is used for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# TSNE from sklearn.manifold is used for dimensionality reduction\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# matplotlib.pyplot is used for creating static, animated, and interactive visualizations in Python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SentenceTransformer is used for training and using transformer models for generating sentence embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# tqdm is used to make loops show a smart progress meter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# torch is the main package in PyTorch, it provides a multi-dimensional array with support for autograd operations like backward()\n",
    "import torch\n",
    "\n",
    "# AutoModelForCausalLM, AutoTokenizer, pipeline are from the transformers library by Hugging Face which provides state-of-the-art machine learning models like BERT, GPT-2, etc.\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# euclidean distance and cosine distance\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# random generator for the last figure\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input folder\n",
    "docInputFolder = \"./short__2\"\n",
    "\n",
    "# name of file with generated requirements\n",
    "generatedFile = \"./output_generated_mini.xlsx\"\n",
    "\n",
    "# embedding model\n",
    "#model = SentenceTransformer(\"sentence-t5-large\")\n",
    "# model = SentenceTransformer(\"sentence-transformers/gtr-t5-xxl\")\n",
    "\n",
    "\n",
    "# generative model\n",
    "modelP = \"microsoft/Phi-3-mini-128k-instruct\"  #\"JoofytheBloofy/T5LargeTest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: List the documents's sections with \"latency\"\n",
    "\n",
    "In the first step, we go through the documents in the folder \"input_standards\" and we extract which sections of these documents contain th word \"latency\". We store the results in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLatencySections(doc):\n",
    "    strSectionTitle = \"\"\n",
    "    dictSections = {}\n",
    "    listLatency = []\n",
    "    skippedSections = 0\n",
    "\n",
    "    doc_result = docx2python(doc,paragraph_styles = True, html=True)\n",
    "\n",
    "    # we iterate over all lines\n",
    "    # look for the section titles (which have the tag <h1>, <h2>, <h3>, etc.)\n",
    "    # then we add the content of each section to the dictionary\n",
    "    # and if there is a word \"latency\" somewhere in the section, we add the section title to the listLatency\n",
    "    for oneLine in tqdm(doc_result.text.split('\\n')):\n",
    "        if \"<h\" in oneLine:\n",
    "            strSectionTitle = oneLine\n",
    "            dictSections[strSectionTitle] = []\n",
    "\n",
    "        if strSectionTitle != \"\":  \n",
    "            dictSections[strSectionTitle].append(oneLine)\n",
    "\n",
    "        keywordsInLine = [\"latency\"]\n",
    "        keywordsInSections = [\"abstract\", \n",
    "                              \"acknowledgements\", \n",
    "                              \"appendix\", \n",
    "                              \"bibliography\", \n",
    "                              \"conclusion\", \n",
    "                              \"definition\", \n",
    "                              \"glossary\",\n",
    "                              \"index\", \n",
    "                              \"introduction\",\n",
    "                              \"references\",\n",
    "                              \"table of contents\", \n",
    "                              \"table of figures\"]\n",
    "\n",
    "        if any(word in oneLine.lower() for word in keywordsInLine) and not any(word in strSectionTitle.lower() for word in keywordsInSections): \n",
    "            listLatency.append(strSectionTitle)\n",
    "        else:\n",
    "            skippedSections += 1\n",
    "            \n",
    "            \n",
    "    # remove the keys from the dictionary if they are not part of the listLatency\n",
    "    # as we want to get only the relevant sections, i.e., the one with the word latency\n",
    "    for key in list(dictSections.keys()):\n",
    "        if key not in listLatency:\n",
    "            del dictSections[key]\n",
    "\n",
    "    \n",
    "    # return the dictionary with the relevant sections\n",
    "    return dictSections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the return list of all the lines in the document\n",
    "lstAllLines = []\n",
    "\n",
    "# for each .docx file in the input folder\n",
    "# extract the sections with latency using the extractLatencySections function\n",
    "# and print the sections\n",
    "for doc in tqdm(os.listdir(docInputFolder)):    \n",
    "\n",
    "    if doc.endswith(\".docx\"):\n",
    "        \n",
    "        # since things can go wrong with the latency library, \n",
    "        # we use a try except block to avoid the program to stop\n",
    "        try: \n",
    "            dictSections = extractLatencySections(os.path.join(docInputFolder, doc))\n",
    "        \n",
    "            # we list the content as a long list of sections \n",
    "            for key in dictSections:\n",
    "\n",
    "                lstOneLine = [key, doc]\n",
    "\n",
    "                for line in dictSections[key]:\n",
    "                    lstOneLine.append(line)\n",
    "                    \n",
    "                lstAllLines.append(lstOneLine)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {doc}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = pd.DataFrame(lstAllLines)\n",
    "\n",
    "dfTest[1].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list with all embeddings for the sections\n",
    "lstEmbeddings = []\n",
    "iCounter = 0\n",
    "\n",
    "for oneLine in tqdm(lstAllLines):\n",
    "\n",
    "    # the content of the section starts on the third position of the list\n",
    "    sentences = oneLine[3:]\n",
    "\n",
    "    # Sentences are encoded by calling model.encode()\n",
    "    #embeddings = model.encode(sentences)\n",
    "        \n",
    "    lstOneLine = [oneLine[0], oneLine[1], str(sentences).replace(\"$\", \"_\").replace(\"\\n\", \"_\")]\n",
    "\n",
    "    lstEmbeddings.append(lstOneLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lstEmbeddings to an xlsx file topic_relevant.xlsx\n",
    "df = pd.DataFrame(lstEmbeddings, columns=[\"Section\", \"Document\", \"Sentences\"])\n",
    "df.to_excel(\"topic_relevant.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: create requirements based on each of these sections\n",
    "\n",
    "In the last step, we create new requirements based on the sections identified in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the lstDst from the Excel file\n",
    "dfDistances = pd.read_excel(\"./topic_relevant.xlsx\")\n",
    "\n",
    "lstDist = dfDistances.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0)\n",
    "\n",
    "modelInstr = AutoModelForCausalLM.from_pretrained(\n",
    "    modelP, \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\",     \n",
    "    trust_remote_code=True, \n",
    "    attn_implementation='eager',\n",
    ")\n",
    "tokenizerInstr = AutoTokenizer.from_pretrained(modelP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRequirement2(content, section, document, model, tokenizer):\n",
    "    content1 = content.split(\",\")\n",
    "    content1 = [x for x in content1[1:] if x not in ['', \" ''\", \" '']\"]]\n",
    "    content_str = \" \".join(content1)\n",
    "\n",
    "    # check if the text contains the word Figure\n",
    "    # if it does, then we add a footnote before to warn the user\n",
    "    if \"Figure\" in content_str:\n",
    "        #strContent = f\"Based on this : {content_str}. Write the requirement in the following format 'The system shall '. Start with '* This part of the standard contains a figure, the generated requirement can be inaccurate, please consult the original text for details.' \"\n",
    "        strContent = f\"Based on this : {content_str}, write the requirement about {section} from {document}. Add this text at the beginning: '* This part of the standard contains a figure, the generated requirement can be inaccurate, please consult the original text for details.' \"\n",
    "    # the same for tables, at least the ones that we can identify\n",
    "    elif \"Table \" in content_str[:10]:\n",
    "        #strContent = f\"Summarize this table. {content_str}. Based on this summary, write the requirement in the following format 'The system shall '. Start with '* This part of the standard contains a table, the generated requirement can be inaccurate, please consult the original text for details.' \"\n",
    "        strContent = f\"Summarize this table. {content_str}. Based on this summary, write a requirement about {section} from {document}. Add this text at the beginning '* This part of the standard contains a table, the generated requirement can be inaccurate, please consult the original text for details.' \"\n",
    "    \n",
    "    # and for the empty text, e.g., when the word latency is only in the title\n",
    "    # we do not generate anything and warn the user\n",
    "    elif len(content_str) < 2:\n",
    "        return \"This section is empty. The word latency is probably only in a section title\"\n",
    "    # otherwise, we generate the requirement\n",
    "    else: \n",
    "        #strContent = f\"Based on this : {content_str}. Write the requirement in the following format 'The system shall ' \"\n",
    "        strContent = f\"Write a requirement about {section} from {document} based on this: {content_str}. \"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": strContent},\n",
    "    ]\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0,\n",
    "    }\n",
    "\n",
    "    output = pipe(messages, **generation_args)\n",
    "    \n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstGenerated = []\n",
    "iCounter = 0\n",
    "\n",
    "# we generate new requirements for the partially covered ones\n",
    "if len(lstDist) > 0:\n",
    "    for oneLine in tqdm(lstDist):\n",
    "        if (len(oneLine[2]) < 4095):\n",
    "            strRequirement = createRequirement2(oneLine[2], oneLine[0], oneLine[1], modelInstr, tokenizerInstr)\n",
    "            lstGenerated.append([oneLine[0], oneLine[1], oneLine[2], strRequirement])\n",
    "            dfOutput = pd.DataFrame(lstGenerated, columns=[\"Section\", \"Document\", \"Content\", \"Generated requirement\"])\n",
    "            dfOutput.to_excel(generatedFile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOutput = pd.DataFrame(lstGenerated, columns=[\"Section\", \"Document\", \"Content\", \"Generated requirement\"])\n",
    "dfOutput.to_excel(generatedFile, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
