{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3REQ system\n",
    "\n",
    "Requirement analysis system.\n",
    "\n",
    "The flow is presented in the following figure: \n",
    "\n",
    "<img src=\"flow.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "Summary:\n",
    "1. Extract sections that contain words like \"latency\"\n",
    "2. Find if they resemble requirements or not\n",
    "3. Check if they are specific kinds of requirements (like signalling)\n",
    "4. Write new requirements based on the text in these sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "# docx2python is used to extract text, images, tables, and other data from .docx files\n",
    "from docx2python import docx2python\n",
    "\n",
    "# os module provides functions for interacting with the operating system\n",
    "import os\n",
    "\n",
    "# numpy is used for mathematical operations on large, multi-dimensional arrays and matrices\n",
    "import numpy as np\n",
    "\n",
    "# pandas is used for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# TSNE from sklearn.manifold is used for dimensionality reduction\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# matplotlib.pyplot is used for creating static, animated, and interactive visualizations in Python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SentenceTransformer is used for training and using transformer models for generating sentence embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# tqdm is used to make loops show a smart progress meter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# torch is the main package in PyTorch, it provides a multi-dimensional array with support for autograd operations like backward()\n",
    "import torch\n",
    "\n",
    "# AutoModelForCausalLM, AutoTokenizer, pipeline are from the transformers library by Hugging Face which provides state-of-the-art machine learning models like BERT, GPT-2, etc.\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# euclidean distance and cosine distance\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# random generator for the last figure\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: List the documents's sections with \"latency\"\n",
    "\n",
    "In the first step, we go through the documents in the folder \"input_standards\" and we extract which sections of these documents contain th word \"latency\". We store the results in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLatencySections(doc):\n",
    "    strSectionTitle = \"\"\n",
    "    dictSections = {}\n",
    "    listLatency = []\n",
    "    skippedSections = 0\n",
    "\n",
    "    doc_result = docx2python(doc,paragraph_styles = True, html=True)\n",
    "\n",
    "    # we iterate over all lines\n",
    "    # look for the section titles (which have the tag <h1>, <h2>, <h3>, etc.)\n",
    "    # then we add the content of each section to the dictionary\n",
    "    # and if there is a word \"latency\" somewhere in the section, we add the section title to the listLatency\n",
    "    for oneLine in tqdm(doc_result.text.split('\\n')):\n",
    "        if \"<h\" in oneLine:\n",
    "            strSectionTitle = oneLine\n",
    "            dictSections[strSectionTitle] = []\n",
    "\n",
    "        if strSectionTitle != \"\":  \n",
    "            dictSections[strSectionTitle].append(oneLine)\n",
    "\n",
    "        keywordsInLine = [\"latency\"]\n",
    "        keywordsInSections = [\"references\", \n",
    "                              \"introduction\", \n",
    "                              \"definition\", \n",
    "                              \"abstract\", \n",
    "                              \"conclusion\", \n",
    "                              \"description\", \n",
    "                              \"acknowledgements\", \n",
    "                              \"annex\", \n",
    "                              \"appendix\", \n",
    "                              \"table of contents\", \n",
    "                              \"table of figures\", \n",
    "                              \"table of tables\", \n",
    "                              \"bibliography\", \n",
    "                              \"index\", \n",
    "                              \"glossary\", \n",
    "                              \"list of figures\", \n",
    "                              \"list of tables\", \n",
    "                              \"list of abbreviations\", \n",
    "                              \"list of symbols\", \n",
    "                              \"list of terms\", \n",
    "                              \"list of equations\", \n",
    "                              \"list of algorithms\", \n",
    "                              \"list of acronyms\", \n",
    "                              \"list of illustrations\", \n",
    "                              \"list of appendices\"]\n",
    "\n",
    "        if any(word in oneLine.lower() for word in keywordsInLine) and not any(word in strSectionTitle.lower() for word in keywordsInSections): \n",
    "            listLatency.append(strSectionTitle)\n",
    "        else:\n",
    "            skippedSections += 1\n",
    "            \n",
    "            \n",
    "    # remove the keys from the dictionary if they are not part of the listLatency\n",
    "    # as we want to get only the relevant sections, i.e., the one with the word latency\n",
    "    for key in list(dictSections.keys()):\n",
    "        if key not in listLatency:\n",
    "            del dictSections[key]\n",
    "\n",
    "    # print(\"Skipped sections: \", skippedSections)\n",
    "\n",
    "    # return the dictionary with the relevant sections\n",
    "    return dictSections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                        | 0/5 [00:00<?, ?it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4620/4620 [00:00<00:00, 1469787.96it/s]\u001b[A\n",
      " 20%|███████████████████▏                                                                            | 1/5 [00:00<00:02,  1.81it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 9286/9286 [00:00<00:00, 2214733.71it/s]\u001b[A\n",
      " 40%|██████████████████████████████████████▍                                                         | 2/5 [00:01<00:02,  1.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 25140/25140 [00:00<00:00, 2071118.84it/s]\u001b[A\n",
      " 60%|█████████████████████████████████████████████████████████▌                                      | 3/5 [00:04<00:03,  1.83s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2739/2739 [00:00<00:00, 1954107.61it/s]\u001b[A\n",
      " 80%|████████████████████████████████████████████████████████████████████████████▊                   | 4/5 [00:04<00:01,  1.23s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2985/2985 [00:00<00:00, 2053468.50it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "docInputFolder = \"./23_few__5\"\n",
    "\n",
    "# this is the return list of all the lines in the document\n",
    "lstAllLines = []\n",
    "\n",
    "# for each .docx file in the input folder\n",
    "# extract the sections with latency using the extractLatencySections function\n",
    "# and print the sections\n",
    "for doc in tqdm(os.listdir(docInputFolder)):    \n",
    "\n",
    "    if doc.endswith(\".docx\"):\n",
    "        #print(f\"Processing {doc}\")\n",
    "\n",
    "        # since things can go wrong with the latency library, \n",
    "        # we use a try except block to avoid the program to stop\n",
    "        try: \n",
    "            dictSections = extractLatencySections(os.path.join(docInputFolder, doc))\n",
    "        \n",
    "            # we list the content\n",
    "            # as a long list of sections \n",
    "            for key in dictSections:\n",
    "\n",
    "                lstOneLine = [key, doc]\n",
    "\n",
    "                for line in dictSections[key]:\n",
    "                    lstOneLine.append(line)\n",
    "                    \n",
    "                lstAllLines.append(lstOneLine)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {doc}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [09:32<00:00, 44.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# choose the right model\n",
    "# in order of size -- the last one is only for A6000 :) \n",
    "\n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model = SentenceTransformer(\"sentence-t5-large\")\n",
    "# model = SentenceTransformer(\"sentence-transformers/gtr-t5-xxl\")\n",
    "\n",
    "# list with all embeddings for the sections\n",
    "lstEmbeddings = []\n",
    "iCounter = 0\n",
    "\n",
    "for oneLine in tqdm(lstAllLines):\n",
    "\n",
    "    # the content of the section starts on the third position of the list\n",
    "    sentences = oneLine[3:]\n",
    "\n",
    "    # Sentences are encoded by calling model.encode()\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Print the average embeddings for all the sentences \n",
    "    # in this section\n",
    "    avg_embeddings = embeddings.mean(axis=0)\n",
    "    \n",
    "    lstOneLine = [oneLine[0], oneLine[1], 2, str(sentences).replace(\"$\", \"_\").replace(\"\\n\", \"_\"), avg_embeddings]\n",
    "\n",
    "    lstEmbeddings.append(lstOneLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lstEmbeddings to an xlsx file topic_relevant.xlsx\n",
    "df = pd.DataFrame(lstEmbeddings, columns=[\"Section\", \"Document\", \"Category\", \"Sentences\", \"Embeddings\"])\n",
    "df.to_excel(\"topic_relevant.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Requirements classes\n",
    "\n",
    "Checking whether these are signalling, payload, etc. requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 38/38 [00:25<00:00,  1.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# read the requirements from the excel file requirements.xlsx, worksheet LR\n",
    "df = pd.read_excel(\"req_classes.xlsx\", sheet_name=\"LR\")\n",
    "\n",
    "# convert to list\n",
    "lstRequirements = df.values.tolist()\n",
    "lstRequirements[0]\n",
    "\n",
    "# now we calculate the embeddings for each of these requirements\n",
    "lstEmbeddingsReq = []\n",
    "\n",
    "for oneLine in tqdm(lstRequirements):\n",
    "    \n",
    "        # the content of the section starts on the third position of the list\n",
    "        sentences = oneLine[1]\n",
    "    \n",
    "        # Sentences are encoded by calling model.encode()\n",
    "        embeddings = model.encode(sentences)\n",
    "        \n",
    "        # Print the average embeddings for all the sentences \n",
    "        # in this section\n",
    "        avg_embedding = embeddings\n",
    "        \n",
    "        lstOneLine = [oneLine[0], 'latency', oneLine[1], oneLine[1], avg_embedding]\n",
    "    \n",
    "        lstEmbeddingsReq.append(lstOneLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 860.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# now we calculate the euclidean distance between the requirements and the sections\n",
    "# that are relevant\n",
    "lstDist = []\n",
    "lstRelevantDist = []\n",
    "\n",
    "for oneLine in tqdm(lstEmbeddings):\n",
    "    for oneLineReq in lstEmbeddingsReq:\n",
    "        # euclidean distance between the two embeddings\n",
    "        dist = distance.cosine(oneLine[4], oneLineReq[4])\n",
    "        lstDist.append([oneLine[0], oneLine[1], oneLineReq[0], dist, oneLine[3]])\n",
    "\n",
    "# now we sort the list by the distance\n",
    "lstDist.sort(key=lambda x: x[2])\n",
    "\n",
    "# and we print them\n",
    "for i in range(len(lstDist)):\n",
    "    # print(f\"Section {lstDist[i][0]} is close to requirement {lstDist[i][2]} with distance {lstDist[i][3]:.2f}\")\n",
    "    # add this to a list\n",
    "    lstRelevantDist.append([lstDist[i][0], lstDist[i][1], lstDist[i][2], lstDist[i][3], lstDist[i][4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the list to an Excel file\n",
    "dfOutput = pd.DataFrame(lstRelevantDist, columns=[\"Section\", \"Document\", \"Requirement\", \"Distance\", \"Content\"])\n",
    "\n",
    "# sort it by section and document\n",
    "dfOutput = dfOutput.sort_values(by=[\"Section\", \"Document\"])\n",
    "\n",
    "# average the distance in dfOutput per section, document and requirement\n",
    "dfOutput[\"Distance\"] = dfOutput[\"Distance\"].astype(float)\n",
    "dfGrouped = dfOutput.groupby([\"Section\", \"Document\", \"Requirement\", \"Content\"])\n",
    "\n",
    "#convert dfGrouped to a dataframe\n",
    "dfGrouped = dfGrouped.agg({\"Distance\": \"mean\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check the minimum distance per section and document\n",
    "dfGrouped = dfGrouped.sort_values(by=[\"Section\", \"Document\", \"Distance\"])\n",
    "\n",
    "dfGrouped = dfGrouped.groupby([\"Section\", \"Document\"]).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# now we turn this into a list of lists\n",
    "lstRelevantDistGroup = dfGrouped.values.tolist()\n",
    "\n",
    "print(len(lstRelevantDistGroup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe and save to excel lstRelevantDist\n",
    "dfRelevantDist = pd.DataFrame(lstRelevantDistGroup, columns=[\"Section\", \"Document\", \"Requirement\", \"Content\", \"Distance\"])\n",
    "\n",
    "dfRelevantDist.to_excel(\"./classified_requirements.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Check if the requirements exist in the database\n",
    "\n",
    "In this step, we check if the text that we identified so far is covered by the requirements that exist in the database. We use the sentence transformers to get the embeddings of the text and then we compare them to the existin sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the requirements from the Excel file\n",
    "dfRelevantSections = pd.read_excel(\"./classified_requirements.xlsx\")\n",
    "\n",
    "lstRelevantSections = dfRelevantSections.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:14<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "# model = SentenceTransformer(\"sentence-t5-large\")\n",
    "# model = SentenceTransformer(\"sentence-transformers/gtr-t5-xxl\")\n",
    "\n",
    "# list with all embeddings for the sections\n",
    "lstRelevantEmbeddings = []\n",
    "iCounter = 0\n",
    "\n",
    "for oneLine in tqdm(lstRelevantSections):\n",
    "\n",
    "    # the content of the section starts on the third position of the list\n",
    "    sentences = oneLine[3]\n",
    "\n",
    "    # Sentences are encoded by calling model.encode()\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Print the average embeddings for all the sentences \n",
    "    # in this section\n",
    "    # avg_embedding = np.mean(embeddings, axis=0)\n",
    "    \n",
    "    lstOneLine = [oneLine[0], oneLine[1], 2, str(sentences).replace(\"$\", \"_\").replace(\"\\n\", \"_\"), embeddings]\n",
    "\n",
    "    lstRelevantEmbeddings.append(lstOneLine)\n",
    "\n",
    "    iCounter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, read the requirements from the requirements database, file 20_requirements.xlsx\n",
    "dfTRequirements = pd.read_excel(\"existing_requirements.xlsx\")\n",
    "\n",
    "# convert to list\n",
    "lstTRequirements = dfTRequirements.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:08<00:00,  2.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# make the embeddings\n",
    "# model = SentenceTransformer(\"sentence-t5-large\")\n",
    "\n",
    "# list with all embeddings for the sections\n",
    "lstTRequirementsEmbeddings = []\n",
    "iCounter = 0\n",
    "\n",
    "for oneLine in tqdm(lstTRequirements):\n",
    "\n",
    "    # the content of the section starts on the requirement text\n",
    "    sentences = oneLine[1]\n",
    "\n",
    "    # Sentences are encoded by calling model.encode()\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Print the average embeddings for all the sentences \n",
    "    # in this section\n",
    "    #avg_embedding = np.mean(embeddings, axis=0)\n",
    "    \n",
    "    lstOneLine = [oneLine[0], oneLine[1], 2, str(sentences).replace(\"$\", \"_\").replace(\"\\n\", \"_\"), embeddings]\n",
    "\n",
    "    lstTRequirementsEmbeddings.append(lstOneLine)\n",
    "\n",
    "    iCounter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the embeddings for all the lstTRequirementsEmbeddings\n",
    "lstTRequirementsEmbeddingsNP = np.array([x[4] for x in lstTRequirementsEmbeddings])\n",
    "\n",
    "tRequirementsAvgEmbeddings = np.mean(lstTRequirementsEmbeddingsNP, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 12554.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# now, calculate the average distance of all the relevant sections to the average requirements\n",
    "lstDist = []\n",
    "\n",
    "for oneLine in tqdm(lstRelevantEmbeddings):\n",
    "    # euclidean distance between the two embeddings\n",
    "    dist = distance.cosine(oneLine[4], tRequirementsAvgEmbeddings)\n",
    "    lstDist.append([oneLine[0], oneLine[1], dist, oneLine[3]])\n",
    "\n",
    "# now we sort the list by the distance\n",
    "lstDist.sort(key=lambda x: x[3])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: create requirements based on each of these sections\n",
    "\n",
    "In the last step, we create new requirements based on the sections identified in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards:  50%|██████████████████████████████████▌                                  | 3/6 [02:59<02:59, 59.93s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 76.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m modelInstr \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#\"microsoft/Phi-3-mini-128k-instruct\",\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/Phi-3-medium-128k-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meager\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m tokenizerInstr \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/Phi-3-mini-4k-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:558\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3677\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3668\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3669\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3670\u001b[0m     (\n\u001b[1;32m   3671\u001b[0m         model,\n\u001b[1;32m   3672\u001b[0m         missing_keys,\n\u001b[1;32m   3673\u001b[0m         unexpected_keys,\n\u001b[1;32m   3674\u001b[0m         mismatched_keys,\n\u001b[1;32m   3675\u001b[0m         offload_index,\n\u001b[1;32m   3676\u001b[0m         error_msgs,\n\u001b[0;32m-> 3677\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3684\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3685\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3688\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3689\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3693\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3695\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3696\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:4104\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4100\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4101\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4102\u001b[0m                 )\n\u001b[1;32m   4103\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4104\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4107\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4109\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4110\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4111\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4112\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4113\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4115\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4116\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4117\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4118\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4119\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4120\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4121\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:886\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    875\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m is_quantized\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mrequires_parameters_quantization)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    884\u001b[0m ):\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 886\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/modeling.py:399\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    397\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 399\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 76.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "\n",
    "modelInstr = AutoModelForCausalLM.from_pretrained(\n",
    "    #\"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    \"microsoft/Phi-3-medium-128k-instruct\",\n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    "    attn_implementation='eager',\n",
    ")\n",
    "tokenizerInstr = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-medium-128k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRequirement(content, type, model, tokenizer):\n",
    "    content1 = content.split(\",\")\n",
    "    content1 = [x for x in content1[1:] if x not in ['', \" ''\", \" '']\"]]\n",
    "    content_str = \" \".join(content1)\n",
    "\n",
    "    # this is about signalling, payload, c/c. \n",
    "    # typeStr = type.split(\"_\")[1]\n",
    "    \n",
    "    # strContent = f\"Based on this : {content_str}. Write the requirement in the following format 'The {typeStr} of the system shall ' \"\n",
    "    strContent = f\"Based on this : {content_str}. Write the requirement in the following format 'The system shall ' \"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": strContent},\n",
    "    ]\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "\n",
    "    output = pipe(messages, **generation_args)\n",
    "    \n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstGenerated = []\n",
    "iCounter = 0\n",
    "\n",
    "# we generate new requirements for the partially covered ones\n",
    "if len(lstDist) > 0:\n",
    "    for oneLine in tqdm(lstDist):\n",
    "        if (len(oneLine[3]) < 4095):\n",
    "            strRequirement = createRequirement(oneLine[3], oneLine[2], modelInstr, tokenizerInstr)\n",
    "            lstGenerated.append([oneLine[0], oneLine[1], oneLine[2], oneLine[3], strRequirement])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOutput = pd.DataFrame(lstGenerated, columns=[\"Section\", \"Document\",  \"Distance\", \"Content\", \"Generated requirement\"])\n",
    "dfOutput.to_excel(\"./output_generated.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
