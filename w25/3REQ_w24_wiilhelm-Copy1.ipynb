{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3REQ system\n",
    "\n",
    "Requirement analysis system.\n",
    "\n",
    "The flow is presented in the following figure: \n",
    "\n",
    "<img src=\"flow.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "Summary:\n",
    "1. Extract sections that contain words like \"latency\"\n",
    "2. Find if they resemble requirements or not\n",
    "3. Check if they are specific kinds of requirements (like signalling)\n",
    "4. Write new requirements based on the text in these sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wilhelmmeding/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "# docx2python is used to extract text, images, tables, and other data from .docx files\n",
    "from docx2python import docx2python\n",
    "\n",
    "# os module provides functions for interacting with the operating system\n",
    "import os\n",
    "\n",
    "# numpy is used for mathematical operations on large, multi-dimensional arrays and matrices\n",
    "import numpy as np\n",
    "\n",
    "# pandas is used for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# TSNE from sklearn.manifold is used for dimensionality reduction\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# matplotlib.pyplot is used for creating static, animated, and interactive visualizations in Python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SentenceTransformer is used for training and using transformer models for generating sentence embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# tqdm is used to make loops show a smart progress meter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# torch is the main package in PyTorch, it provides a multi-dimensional array with support for autograd operations like backward()\n",
    "import torch\n",
    "\n",
    "# AutoModelForCausalLM, AutoTokenizer, pipeline are from the transformers library by Hugging Face which provides state-of-the-art machine learning models like BERT, GPT-2, etc.\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# euclidean distance and cosine distance\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# random generator for the last figure\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: List the documents's sections with \"latency\"\n",
    "\n",
    "In the first step, we go through the documents in the folder \"input_standards\" and we extract which sections of these documents contain th word \"latency\". We store the results in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLatencySections(doc):\n",
    "    strSectionTitle = \"\"\n",
    "    dictSections = {}\n",
    "    listLatency = []\n",
    "    skippedSections = 0\n",
    "\n",
    "    doc_result = docx2python(doc,paragraph_styles = True, html=True)\n",
    "\n",
    "    # we iterate over all lines\n",
    "    # look for the section titles (which have the tag <h1>, <h2>, <h3>, etc.)\n",
    "    # then we add the content of each section to the dictionary\n",
    "    # and if there is a word \"latency\" somewhere in the section, we add the section title to the listLatency\n",
    "    for oneLine in tqdm(doc_result.text.split('\\n')):\n",
    "        if \"<h\" in oneLine:\n",
    "            strSectionTitle = oneLine\n",
    "            dictSections[strSectionTitle] = []\n",
    "\n",
    "        if strSectionTitle != \"\":  \n",
    "            dictSections[strSectionTitle].append(oneLine)\n",
    "\n",
    "        keywordsInLine = [\"latency\"]\n",
    "        keywordsInSections = [\"references\", \n",
    "                              \"introduction\", \n",
    "                              \"definition\", \n",
    "                              \"abstract\", \n",
    "                              \"conclusion\", \n",
    "                              \"description\", \n",
    "                              \"acknowledgements\", \n",
    "                              \"annex\", \n",
    "                              \"appendix\", \n",
    "                              \"table of contents\", \n",
    "                              \"table of figures\", \n",
    "                              \"table of tables\", \n",
    "                              \"bibliography\", \n",
    "                              \"index\", \n",
    "                              \"glossary\", \n",
    "                              \"list of figures\", \n",
    "                              \"list of tables\", \n",
    "                              \"list of abbreviations\", \n",
    "                              \"list of symbols\", \n",
    "                              \"list of terms\", \n",
    "                              \"list of equations\", \n",
    "                              \"list of algorithms\", \n",
    "                              \"list of acronyms\", \n",
    "                              \"list of illustrations\", \n",
    "                              \"list of appendices\"]\n",
    "\n",
    "        if any(word in oneLine.lower() for word in keywordsInLine) and not any(word in strSectionTitle.lower() for word in keywordsInSections): \n",
    "            listLatency.append(strSectionTitle)\n",
    "        else:\n",
    "            skippedSections += 1\n",
    "            \n",
    "            \n",
    "    # remove the keys from the dictionary if they are not part of the listLatency\n",
    "    # as we want to get only the relevant sections, i.e., the one with the word latency\n",
    "    for key in list(dictSections.keys()):\n",
    "        if key not in listLatency:\n",
    "            del dictSections[key]\n",
    "\n",
    "    # print(\"Skipped sections: \", skippedSections)\n",
    "\n",
    "    # return the dictionary with the relevant sections\n",
    "    return dictSections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                       | 0/36 [00:00<?, ?it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 14280/14280 [00:00<00:00, 2449980.00it/s]\u001b[A\n",
      "  3%|██▋                                                                                            | 1/36 [00:01<00:43,  1.23s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 25005/25005 [00:00<00:00, 2357457.55it/s]\u001b[A\n",
      "  6%|█████▎                                                                                         | 2/36 [00:03<01:06,  1.94s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2171/2171 [00:00<00:00, 2062476.55it/s]\u001b[A\n",
      "  8%|███████▉                                                                                       | 3/36 [00:03<00:37,  1.15s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 17829/17829 [00:00<00:00, 2007092.33it/s]\u001b[A\n",
      " 11%|██████████▌                                                                                    | 4/36 [00:05<00:47,  1.48s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20633/20633 [00:00<00:00, 2427383.44it/s]\u001b[A\n",
      " 14%|█████████████▏                                                                                 | 5/36 [00:07<00:46,  1.50s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 9246/9246 [00:00<00:00, 2276921.96it/s]\u001b[A\n",
      " 17%|███████████████▊                                                                               | 6/36 [00:08<00:39,  1.31s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 9996/9996 [00:00<00:00, 2561947.01it/s]\u001b[A\n",
      " 19%|██████████████████▍                                                                            | 7/36 [00:09<00:32,  1.14s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1011/1011 [00:00<00:00, 1895592.91it/s]\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 17526/17526 [00:00<00:00, 2223917.59it/s]\u001b[A\n",
      " 25%|███████████████████████▊                                                                       | 9/36 [00:11<00:28,  1.07s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1025/1025 [00:00<00:00, 1932207.46it/s]\u001b[A\n",
      " 28%|██████████████████████████                                                                    | 10/36 [00:11<00:21,  1.20it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4646/4646 [00:00<00:00, 1994140.03it/s]\u001b[A\n",
      " 31%|████████████████████████████▋                                                                 | 11/36 [00:11<00:17,  1.41it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 8914/8914 [00:00<00:00, 2069524.29it/s]\u001b[A\n",
      " 33%|███████████████████████████████▎                                                              | 12/36 [00:12<00:19,  1.25it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 8230/8230 [00:00<00:00, 1986369.08it/s]\u001b[A\n",
      " 36%|█████████████████████████████████▉                                                            | 13/36 [00:13<00:18,  1.26it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 9896/9896 [00:00<00:00, 1788933.38it/s]\u001b[A\n",
      " 39%|████████████████████████████████████▌                                                         | 14/36 [00:14<00:18,  1.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4337/4337 [00:00<00:00, 2195883.20it/s]\u001b[A\n",
      " 42%|███████████████████████████████████████▏                                                      | 15/36 [00:14<00:14,  1.43it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 25379/25379 [00:00<00:00, 2155369.66it/s]\u001b[A\n",
      " 44%|█████████████████████████████████████████▊                                                    | 16/36 [00:16<00:22,  1.15s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 5753/5753 [00:00<00:00, 2033355.60it/s]\u001b[A\n",
      " 47%|████████████████████████████████████████████▍                                                 | 17/36 [00:17<00:18,  1.03it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 8827/8827 [00:00<00:00, 2142913.78it/s]\u001b[A\n",
      " 50%|███████████████████████████████████████████████                                               | 18/36 [00:18<00:16,  1.07it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 5655/5655 [00:00<00:00, 2086819.38it/s]\u001b[A\n",
      " 53%|█████████████████████████████████████████████████▌                                            | 19/36 [00:18<00:13,  1.27it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2991/2991 [00:00<00:00, 2030947.59it/s]\u001b[A\n",
      " 56%|████████████████████████████████████████████████████▏                                         | 20/36 [00:19<00:10,  1.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 6588/6588 [00:00<00:00, 2153204.61it/s]\u001b[A\n",
      " 58%|██████████████████████████████████████████████████████▊                                       | 21/36 [00:19<00:10,  1.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 20414/20414 [00:00<00:00, 1932395.72it/s]\u001b[A\n",
      " 61%|█████████████████████████████████████████████████████████▍                                    | 22/36 [00:22<00:15,  1.13s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 9080/9080 [00:00<00:00, 2385784.65it/s]\u001b[A\n",
      " 64%|████████████████████████████████████████████████████████████                                  | 23/36 [00:23<00:14,  1.09s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4928/4928 [00:00<00:00, 2275878.67it/s]\u001b[A\n",
      " 67%|██████████████████████████████████████████████████████████████▋                               | 24/36 [00:23<00:11,  1.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 69254/69254 [00:00<00:00, 2166265.16it/s]\u001b[A\n",
      " 69%|█████████████████████████████████████████████████████████████████▎                            | 25/36 [00:29<00:26,  2.40s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 74539/74539 [00:00<00:00, 2213641.47it/s]\u001b[A\n",
      " 72%|███████████████████████████████████████████████████████████████████▉                          | 26/36 [00:36<00:39,  3.93s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 19714/19714 [00:00<00:00, 2270110.62it/s]\u001b[A\n",
      " 75%|██████████████████████████████████████████████████████████████████████▌                       | 27/36 [00:38<00:29,  3.29s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2553/2553 [00:00<00:00, 1601085.24it/s]\u001b[A\n",
      " 78%|█████████████████████████████████████████████████████████████████████████                     | 28/36 [00:39<00:19,  2.39s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2043/2043 [00:00<00:00, 1112130.18it/s]\u001b[A\n",
      " 81%|███████████████████████████████████████████████████████████████████████████▋                  | 29/36 [00:39<00:12,  1.76s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 9713/9713 [00:00<00:00, 2384086.77it/s]\u001b[A\n",
      " 83%|██████████████████████████████████████████████████████████████████████████████▎               | 30/36 [00:40<00:09,  1.55s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1587/1587 [00:00<00:00, 2000709.48it/s]\u001b[A\n",
      " 86%|████████████████████████████████████████████████████████████████████████████████▉             | 31/36 [00:40<00:05,  1.14s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4620/4620 [00:00<00:00, 1713929.28it/s]\u001b[A\n",
      " 89%|███████████████████████████████████████████████████████████████████████████████████▌          | 32/36 [00:40<00:03,  1.10it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 9286/9286 [00:00<00:00, 2243307.62it/s]\u001b[A\n",
      " 92%|██████████████████████████████████████████████████████████████████████████████████████▏       | 33/36 [00:42<00:03,  1.04s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 25140/25140 [00:00<00:00, 2026576.51it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████████████████████████████████████████████████████▊     | 34/36 [00:44<00:02,  1.49s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2739/2739 [00:00<00:00, 1973239.21it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████████████████▍  | 35/36 [00:45<00:01,  1.14s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2985/2985 [00:00<00:00, 2255855.39it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 36/36 [00:45<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "docInputFolder = \"./23_few__1\"\n",
    "\n",
    "# this is the return list of all the lines in the document\n",
    "lstAllLines = []\n",
    "\n",
    "# for each .docx file in the input folder\n",
    "# extract the sections with latency using the extractLatencySections function\n",
    "# and print the sections\n",
    "for doc in tqdm(os.listdir(docInputFolder)):    \n",
    "\n",
    "    if doc.endswith(\".docx\"):\n",
    "        #print(f\"Processing {doc}\")\n",
    "\n",
    "        # since things can go wrong with the latency library, \n",
    "        # we use a try except block to avoid the program to stop\n",
    "        try: \n",
    "            dictSections = extractLatencySections(os.path.join(docInputFolder, doc))\n",
    "        \n",
    "            # we list the content\n",
    "            # as a long list of sections \n",
    "            for key in dictSections:\n",
    "\n",
    "                lstOneLine = [key, doc]\n",
    "\n",
    "                for line in dictSections[key]:\n",
    "                    lstOneLine.append(line)\n",
    "                    \n",
    "                lstAllLines.append(lstOneLine)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {doc}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [01:46<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# choose the right model\n",
    "# in order of size -- the last one is only for A6000 :) \n",
    "\n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model = SentenceTransformer(\"sentence-t5-large\")\n",
    "# model = SentenceTransformer(\"sentence-transformers/gtr-t5-xxl\")\n",
    "\n",
    "# list with all embeddings for the sections\n",
    "lstEmbeddings = []\n",
    "iCounter = 0\n",
    "\n",
    "for oneLine in tqdm(lstAllLines):\n",
    "\n",
    "    # the content of the section starts on the third position of the list\n",
    "    sentences = oneLine[3:]\n",
    "\n",
    "    # Sentences are encoded by calling model.encode()\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Print the average embeddings for all the sentences \n",
    "    # in this section\n",
    "    avg_embeddings = embeddings.mean(axis=0)\n",
    "    \n",
    "    lstOneLine = [oneLine[0], oneLine[1], 2, str(sentences).replace(\"$\", \"_\").replace(\"\\n\", \"_\"), avg_embeddings]\n",
    "\n",
    "    lstEmbeddings.append(lstOneLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lstEmbeddings to an xlsx file topic_relevant.xlsx\n",
    "df = pd.DataFrame(lstEmbeddings, columns=[\"Section\", \"Document\", \"Category\", \"Sentences\", \"Embeddings\"])\n",
    "df.to_excel(\"topic_relevant.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Requirements classes\n",
    "\n",
    "Checking whether these are signalling, payload, etc. requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 38/38 [00:00<00:00, 108.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# read the requirements from the excel file requirements.xlsx, worksheet LR\n",
    "df = pd.read_excel(\"req_classes.xlsx\", sheet_name=\"LR\")\n",
    "\n",
    "# convert to list\n",
    "lstRequirements = df.values.tolist()\n",
    "lstRequirements[0]\n",
    "\n",
    "# now we calculate the embeddings for each of these requirements\n",
    "lstEmbeddingsReq = []\n",
    "\n",
    "for oneLine in tqdm(lstRequirements):\n",
    "    \n",
    "        # the content of the section starts on the third position of the list\n",
    "        sentences = oneLine[1]\n",
    "    \n",
    "        # Sentences are encoded by calling model.encode()\n",
    "        embeddings = model.encode(sentences)\n",
    "        \n",
    "        # Print the average embeddings for all the sentences \n",
    "        # in this section\n",
    "        avg_embedding = embeddings\n",
    "        \n",
    "        lstOneLine = [oneLine[0], 'latency', oneLine[1], oneLine[1], avg_embedding]\n",
    "    \n",
    "        lstEmbeddingsReq.append(lstOneLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 1679.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# now we calculate the euclidean distance between the requirements and the sections\n",
    "# that are relevant\n",
    "lstDist = []\n",
    "lstRelevantDist = []\n",
    "\n",
    "for oneLine in tqdm(lstEmbeddings):\n",
    "    for oneLineReq in lstEmbeddingsReq:\n",
    "        # euclidean distance between the two embeddings\n",
    "        dist = distance.cosine(oneLine[4], oneLineReq[4])\n",
    "        lstDist.append([oneLine[0], oneLine[1], oneLineReq[0], dist, oneLine[3]])\n",
    "\n",
    "# now we sort the list by the distance\n",
    "lstDist.sort(key=lambda x: x[2])\n",
    "\n",
    "# and we print them\n",
    "for i in range(len(lstDist)):\n",
    "    # print(f\"Section {lstDist[i][0]} is close to requirement {lstDist[i][2]} with distance {lstDist[i][3]:.2f}\")\n",
    "    # add this to a list\n",
    "    lstRelevantDist.append([lstDist[i][0], lstDist[i][1], lstDist[i][2], lstDist[i][3], lstDist[i][4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the list to an Excel file\n",
    "dfOutput = pd.DataFrame(lstRelevantDist, columns=[\"Section\", \"Document\", \"Requirement\", \"Distance\", \"Content\"])\n",
    "\n",
    "# sort it by section and document\n",
    "dfOutput = dfOutput.sort_values(by=[\"Section\", \"Document\"])\n",
    "\n",
    "# average the distance in dfOutput per section, document and requirement\n",
    "dfOutput[\"Distance\"] = dfOutput[\"Distance\"].astype(float)\n",
    "dfGrouped = dfOutput.groupby([\"Section\", \"Document\", \"Requirement\", \"Content\"])\n",
    "\n",
    "#convert dfGrouped to a dataframe\n",
    "dfGrouped = dfGrouped.agg({\"Distance\": \"mean\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check the minimum distance per section and document\n",
    "dfGrouped = dfGrouped.sort_values(by=[\"Section\", \"Document\", \"Distance\"])\n",
    "\n",
    "dfGrouped = dfGrouped.groupby([\"Section\", \"Document\"]).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "# now we turn this into a list of lists\n",
    "lstRelevantDistGroup = dfGrouped.values.tolist()\n",
    "\n",
    "print(len(lstRelevantDistGroup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe and save to excel lstRelevantDist\n",
    "dfRelevantDist = pd.DataFrame(lstRelevantDistGroup, columns=[\"Section\", \"Document\", \"Requirement\", \"Content\", \"Distance\"])\n",
    "\n",
    "dfRelevantDist.to_excel(\"./classified_requirements.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Check if the requirements exist in the database\n",
    "\n",
    "In this step, we check if the text that we identified so far is covered by the requirements that exist in the database. We use the sentence transformers to get the embeddings of the text and then we compare them to the existin sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the requirements from the Excel file\n",
    "dfRelevantSections = pd.read_excel(\"./classified_requirements.xlsx\")\n",
    "\n",
    "lstRelevantSections = dfRelevantSections.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:02<00:00, 57.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# model = SentenceTransformer(\"sentence-t5-large\")\n",
    "# model = SentenceTransformer(\"sentence-transformers/gtr-t5-xxl\")\n",
    "\n",
    "# list with all embeddings for the sections\n",
    "lstRelevantEmbeddings = []\n",
    "iCounter = 0\n",
    "\n",
    "for oneLine in tqdm(lstRelevantSections):\n",
    "\n",
    "    # the content of the section starts on the third position of the list\n",
    "    sentences = oneLine[3]\n",
    "\n",
    "    # Sentences are encoded by calling model.encode()\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Print the average embeddings for all the sentences \n",
    "    # in this section\n",
    "    # avg_embedding = np.mean(embeddings, axis=0)\n",
    "    \n",
    "    lstOneLine = [oneLine[0], oneLine[1], 2, str(sentences).replace(\"$\", \"_\").replace(\"\\n\", \"_\"), embeddings]\n",
    "\n",
    "    lstRelevantEmbeddings.append(lstOneLine)\n",
    "\n",
    "    iCounter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, read the requirements from the requirements database, file 20_requirements.xlsx\n",
    "dfTRequirements = pd.read_excel(\"existing_requirements.xlsx\")\n",
    "\n",
    "# convert to list\n",
    "lstTRequirements = dfTRequirements.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 96.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# make the embeddings\n",
    "# model = SentenceTransformer(\"sentence-t5-large\")\n",
    "\n",
    "# list with all embeddings for the sections\n",
    "lstTRequirementsEmbeddings = []\n",
    "iCounter = 0\n",
    "\n",
    "for oneLine in tqdm(lstTRequirements):\n",
    "\n",
    "    # the content of the section starts on the requirement text\n",
    "    sentences = oneLine[1]\n",
    "\n",
    "    # Sentences are encoded by calling model.encode()\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Print the average embeddings for all the sentences \n",
    "    # in this section\n",
    "    #avg_embedding = np.mean(embeddings, axis=0)\n",
    "    \n",
    "    lstOneLine = [oneLine[0], oneLine[1], 2, str(sentences).replace(\"$\", \"_\").replace(\"\\n\", \"_\"), embeddings]\n",
    "\n",
    "    lstTRequirementsEmbeddings.append(lstOneLine)\n",
    "\n",
    "    iCounter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the embeddings for all the lstTRequirementsEmbeddings\n",
    "lstTRequirementsEmbeddingsNP = np.array([x[4] for x in lstTRequirementsEmbeddings])\n",
    "\n",
    "tRequirementsAvgEmbeddings = np.mean(lstTRequirementsEmbeddingsNP, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 56660.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# now, calculate the average distance of all the relevant sections to the average requirements\n",
    "lstDist = []\n",
    "\n",
    "for oneLine in tqdm(lstRelevantEmbeddings):\n",
    "    # euclidean distance between the two embeddings\n",
    "    dist = distance.cosine(oneLine[4], tRequirementsAvgEmbeddings)\n",
    "    lstDist.append([oneLine[0], oneLine[1], dist, oneLine[3]])\n",
    "\n",
    "# now we sort the list by the distance\n",
    "lstDist.sort(key=lambda x: x[3])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: create requirements based on each of these sections\n",
    "\n",
    "In the last step, we create new requirements based on the sections identified in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.41it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "\n",
    "modelInstr = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\", \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    "    attn_implementation='eager',\n",
    ")\n",
    "tokenizerInstr = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRequirement(content, type, model, tokenizer):\n",
    "    content1 = content.split(\",\")\n",
    "    content1 = [x for x in content1[1:] if x not in ['', \" ''\", \" '']\"]]\n",
    "    content_str = \" \".join(content1)\n",
    "\n",
    "    # this is about signalling, payload, c/c. \n",
    "    # typeStr = type.split(\"_\")[1]\n",
    "    \n",
    "    # strContent = f\"Based on this : {content_str}. Write the requirement in the following format 'The {typeStr} of the system shall ' \"\n",
    "    strContent = f\"Based on this : {content_str}. Write the requirement in the following format 'The system shall ' \"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": strContent},\n",
    "    ]\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "\n",
    "    output = pipe(messages, **generation_args)\n",
    "    \n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                      | 0/119 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [14:19<00:00,  7.22s/it]\n"
     ]
    }
   ],
   "source": [
    "lstGenerated = []\n",
    "iCounter = 0\n",
    "\n",
    "# we generate new requirements for the partially covered ones\n",
    "if len(lstDist) > 0:\n",
    "    for oneLine in tqdm(lstDist):\n",
    "        if (len(oneLine[3]) < 4095):\n",
    "            strRequirement = createRequirement(oneLine[3], oneLine[2], modelInstr, tokenizerInstr)\n",
    "            lstGenerated.append([oneLine[0], oneLine[1], oneLine[2], oneLine[3], strRequirement])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOutput = pd.DataFrame(lstGenerated, columns=[\"Section\", \"Document\",  \"Distance\", \"Content\", \"Generated requirement\"])\n",
    "dfOutput.to_excel(\"./output_generated.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
