{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3REQ system\n",
    "\n",
    "Requirement analysis system.\n",
    "\n",
    "The flow is presented in the following figure: \n",
    "\n",
    "<img src=\"flow.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "Summary:\n",
    "1. Extract sections that contain words like \"latency\"\n",
    "2. Find if they resemble requirements or not\n",
    "3. Check if they are specific kinds of requirements (like signalling)\n",
    "4. Write new requirements based on the text in these sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only for Miroslaw\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/mnt/d/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wilhelmmeding/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "There was a problem when trying to write in your cache folder (/mnt/d/cache/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "# docx2python is used to extract text, images, tables, and other data from .docx files\n",
    "from docx2python import docx2python\n",
    "\n",
    "# os module provides functions for interacting with the operating system\n",
    "import os\n",
    "\n",
    "# numpy is used for mathematical operations on large, multi-dimensional arrays and matrices\n",
    "import numpy as np\n",
    "\n",
    "# pandas is used for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# TSNE from sklearn.manifold is used for dimensionality reduction\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# matplotlib.pyplot is used for creating static, animated, and interactive visualizations in Python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SentenceTransformer is used for training and using transformer models for generating sentence embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# tqdm is used to make loops show a smart progress meter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# torch is the main package in PyTorch, it provides a multi-dimensional array with support for autograd operations like backward()\n",
    "import torch\n",
    "\n",
    "# AutoModelForCausalLM, AutoTokenizer, pipeline are from the transformers library by Hugging Face which provides state-of-the-art machine learning models like BERT, GPT-2, etc.\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# euclidean distance and cosine distance\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# random generator for the last figure\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: List the documents's sections with \"latency\"\n",
    "\n",
    "In the first step, we go through the documents in the folder \"input_standards\" and we extract which sections of these documents contain th word \"latency\". We store the results in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLatencySections(doc):\n",
    "    strSectionTitle = \"\"\n",
    "    dictSections = {}\n",
    "    listLatency = []\n",
    "    skippedSections = 0\n",
    "\n",
    "    doc_result = docx2python(doc,paragraph_styles = True, html=True)\n",
    "\n",
    "    # we iterate over all lines\n",
    "    # look for the section titles (which have the tag <h1>, <h2>, <h3>, etc.)\n",
    "    # then we add the content of each section to the dictionary\n",
    "    # and if there is a word \"latency\" somewhere in the section, we add the section title to the listLatency\n",
    "    for oneLine in tqdm(doc_result.text.split('\\n')):\n",
    "        if \"<h\" in oneLine:\n",
    "            strSectionTitle = oneLine\n",
    "            dictSections[strSectionTitle] = []\n",
    "\n",
    "        if strSectionTitle != \"\":  \n",
    "            dictSections[strSectionTitle].append(oneLine)\n",
    "\n",
    "        keywordsInLine = [\"latency\"]\n",
    "        keywordsInSections = [\"references\", \"introduction\", \"definition\", \"abstract\", \"conclusion\", \"acknowledgements\", \"appendix\", \"table of contents\", \"table of figures\", \"table of tables\", \"table of contents\", \"table of figures\", \"table of tables\", \"bibliography\", \"index\", \"glossary\", \"list of figures\", \"list of tables\", \"list of abbreviations\", \"list of symbols\", \"list of terms\", \"list of equations\", \"list of algorithms\", \"list of acronyms\", \"list of illustrations\", \"list of appendices\"]\n",
    "\n",
    "        if any(word in oneLine.lower() for word in keywordsInLine) and not any(word in strSectionTitle.lower() for word in keywordsInSections): \n",
    "            listLatency.append(strSectionTitle)\n",
    "        else:\n",
    "            skippedSections += 1\n",
    "            \n",
    "            \n",
    "    # remove the keys from the dictionary if they are not part of the listLatency\n",
    "    # as we want to get only the relevant sections, i.e., the one with the word latency\n",
    "    for key in list(dictSections.keys()):\n",
    "        if key not in listLatency:\n",
    "            del dictSections[key]\n",
    "\n",
    "    # print(\"Skipped sections: \", skippedSections)\n",
    "\n",
    "    # return the dictionary with the relevant sections\n",
    "    return dictSections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                        | 0/4 [00:00<?, ?it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 7996/7996 [00:00<00:00, 2337281.68it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.56it/s]\n"
     ]
    }
   ],
   "source": [
    "docInputFolder = \"./22_standards_temp\"\n",
    "\n",
    "# this is the return list of all the lines in the document\n",
    "lstAllLines = []\n",
    "\n",
    "# for each .docx file in the input folder\n",
    "# extract the sections with latency using the extractLatencySections function\n",
    "# and print the sections\n",
    "for doc in tqdm(os.listdir(docInputFolder)):    \n",
    "\n",
    "    if doc.endswith(\".docx\"):\n",
    "        #print(f\"Processing {doc}\")\n",
    "\n",
    "        # since things can go wrong with the latency library, \n",
    "        # we use a try except block to avoid the program to stop\n",
    "        try: \n",
    "            dictSections = extractLatencySections(os.path.join(docInputFolder, doc))\n",
    "        \n",
    "            # we list the content\n",
    "            # as a long list of sections \n",
    "            for key in dictSections:\n",
    "\n",
    "                lstOneLine = [key, doc]\n",
    "\n",
    "                for line in dictSections[key]:\n",
    "                    lstOneLine.append(line)\n",
    "                    \n",
    "                lstAllLines.append(lstOneLine)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {doc}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name sentence-transformers/sentence-t5-large. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/mnt/d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# choose the right model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# in order of size -- the last one is only for A6000 :) \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model = SentenceTransformer(\"all-MiniLM-L6-v2\")\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence-t5-large\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#model = SentenceTransformer(\"sentence-transformers/gtr-t5-xxl\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# list with all embeddings for the sections\u001b[39;00m\n\u001b[1;32m      9\u001b[0m lstEmbeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:198\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, cache_folder, trust_remote_code, revision, token, use_auth_token)\u001b[0m\n\u001b[1;32m    190\u001b[0m         modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(\n\u001b[1;32m    191\u001b[0m             model_name_or_path,\n\u001b[1;32m    192\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m             trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m    196\u001b[0m         )\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m         modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_auto_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modules, OrderedDict):\n\u001b[1;32m    207\u001b[0m     modules \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;28mstr\u001b[39m(idx), module) \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules)])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1063\u001b[0m, in \u001b[0;36mSentenceTransformer._load_auto_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;124;03mCreates a simple Transformer + Mean Pooling model and returns the modules\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo sentence-transformers model found with name \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Creating a new one with MEAN pooling.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1060\u001b[0m         model_name_or_path\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1062\u001b[0m )\n\u001b[0;32m-> 1063\u001b[0m transformer_model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrust_remote_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrust_remote_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1069\u001b[0m pooling_model \u001b[38;5;241m=\u001b[39m Pooling(transformer_model\u001b[38;5;241m.\u001b[39mget_word_embedding_dimension(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [transformer_model, pooling_model]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:35\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_keys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_seq_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_lower_case\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case \u001b[38;5;241m=\u001b[39m do_lower_case\n\u001b[0;32m---> 35\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     39\u001b[0m     tokenizer_name_or_path \u001b[38;5;28;01mif\u001b[39;00m tokenizer_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model_name_or_path,\n\u001b[1;32m     40\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_args,\n\u001b[1;32m     42\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:928\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    926\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 928\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    930\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:631\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    633\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:686\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    413\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1201\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid repo type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Accepted repo types are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(REPO_TYPES)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1200\u001b[0m storage_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cache_dir, repo_folder_name(repo_id\u001b[38;5;241m=\u001b[39mrepo_id, repo_type\u001b[38;5;241m=\u001b[39mrepo_type))\n\u001b[0;32m-> 1201\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;66;03m# cross platform transcription of filename, to be used as a local file path.\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m relative_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;241m*\u001b[39mfilename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/mnt/d'"
     ]
    }
   ],
   "source": [
    "# choose the right model\n",
    "# in order of size -- the last one is only for A6000 :) \n",
    "\n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model = SentenceTransformer(\"sentence-t5-large\")\n",
    "#model = SentenceTransformer(\"sentence-transformers/gtr-t5-xxl\")\n",
    "\n",
    "# list with all embeddings for the sections\n",
    "lstEmbeddings = []\n",
    "iCounter = 0\n",
    "\n",
    "for oneLine in tqdm(lstAllLines):\n",
    "\n",
    "    # the content of the section starts on the third position of the list\n",
    "    sentences = oneLine[3:]\n",
    "\n",
    "    # Sentences are encoded by calling model.encode()\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Print the average embeddings for all the sentences \n",
    "    # in this section\n",
    "    avg_embeddings = embeddings.mean(axis=0)\n",
    "    \n",
    "    lstOneLine = [oneLine[0], oneLine[1], 2, str(sentences).replace(\"$\", \"_\").replace(\"\\n\", \"_\"), avg_embeddings]\n",
    "\n",
    "    lstEmbeddings.append(lstOneLine)\n",
    "\n",
    "    iCounter += 1\n",
    "    # the code below is for debug only\n",
    "    # in case we want to stop the loop after a certain number of iterations\n",
    "    #if iCounter == 100:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lstEmbeddings to an xlsx file topic_relevant.xlsx\n",
    "df = pd.DataFrame(lstEmbeddings, columns=[\"Section\", \"Document\", \"Category\", \"Sentences\", \"Embeddings\"])\n",
    "df.to_excel(\"topic_relevant.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot these average embeddings using t-SNE\n",
    "\n",
    "# we create a list with the embeddings\n",
    "lstEmbeddingsNP = np.array([x[4] for x in lstEmbeddings])\n",
    "\n",
    "# we use t-SNE to reduce the dimensionality of the embeddings\n",
    "tsne = TSNE(n_components=2, verbose=0, perplexity=12, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(lstEmbeddingsNP)\n",
    "\n",
    "# we plot the t-SNE results\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "plt.scatter(tsne_results[:,0], tsne_results[:,1])\n",
    "\n",
    "# Add labels to each dot\n",
    "for i, label in enumerate([x[0] for x in lstEmbeddings]):\n",
    "    plt.text(tsne_results[i, 0], tsne_results[i, 1], label[3:10])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Requirements classes\n",
    "\n",
    "Checking whether these are signalling, payload, etc. requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the requirements from the excel file requirements.xlsx, worksheet LR\n",
    "df = pd.read_excel(\"req_classes.xlsx\", sheet_name=\"LR\")\n",
    "\n",
    "# convert to list\n",
    "lstRequirements = df.values.tolist()\n",
    "lstRequirements[0]\n",
    "\n",
    "# now we calculate the embeddings for each of these requirements\n",
    "lstEmbeddingsReq = []\n",
    "\n",
    "for oneLine in tqdm(lstRequirements):\n",
    "    \n",
    "        # the content of the section starts on the third position of the list\n",
    "        sentences = oneLine[1]\n",
    "    \n",
    "        # Sentences are encoded by calling model.encode()\n",
    "        embeddings = model.encode(sentences)\n",
    "        \n",
    "        # Print the average embeddings for all the sentences \n",
    "        # in this section\n",
    "        avg_embedding = embeddings\n",
    "        \n",
    "        lstOneLine = [oneLine[0], 'latency', oneLine[1], oneLine[1], avg_embedding]\n",
    "    \n",
    "        lstEmbeddingsReq.append(lstOneLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we calculate the euclidean distance between the requirements and the sections\n",
    "# that are relevant\n",
    "lstDist = []\n",
    "lstRelevantDist = []\n",
    "\n",
    "for oneLine in tqdm(lstEmbeddings):\n",
    "    for oneLineReq in lstEmbeddingsReq:\n",
    "        # euclidean distance between the two embeddings\n",
    "        dist = distance.cosine(oneLine[4], oneLineReq[4])\n",
    "        lstDist.append([oneLine[0], oneLine[1], oneLineReq[0], dist, oneLine[3]])\n",
    "\n",
    "# now we sort the list by the distance\n",
    "lstDist.sort(key=lambda x: x[2])\n",
    "\n",
    "# and we print them\n",
    "for i in range(len(lstDist)):\n",
    "    # print(f\"Section {lstDist[i][0]} is close to requirement {lstDist[i][2]} with distance {lstDist[i][3]:.2f}\")\n",
    "    # add this to a list\n",
    "    lstRelevantDist.append([lstDist[i][0], lstDist[i][1], lstDist[i][2], lstDist[i][3], lstDist[i][4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the list to an Excel file\n",
    "dfOutput = pd.DataFrame(lstRelevantDist, columns=[\"Section\", \"Document\", \"Requirement\", \"Distance\", \"Content\"])\n",
    "\n",
    "# sort it by section and document\n",
    "dfOutput = dfOutput.sort_values(by=[\"Section\", \"Document\"])\n",
    "\n",
    "# average the distance in dfOutput per section, document and requirement\n",
    "dfOutput[\"Distance\"] = dfOutput[\"Distance\"].astype(float)\n",
    "dfGrouped = dfOutput.groupby([\"Section\", \"Document\", \"Requirement\", \"Content\"])\n",
    "\n",
    "#convert dfGrouped to a dataframe\n",
    "dfGrouped = dfGrouped.agg({\"Distance\": \"mean\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check the minimum distance per section and document\n",
    "dfGrouped = dfGrouped.sort_values(by=[\"Section\", \"Document\", \"Distance\"])\n",
    "\n",
    "dfGrouped = dfGrouped.groupby([\"Section\", \"Document\"]).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we turn this into a list of lists\n",
    "lstRelevantDistGroup = dfGrouped.values.tolist()\n",
    "\n",
    "print(len(lstRelevantDistGroup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe and save to excel lstRelevantDist\n",
    "dfRelevantDist = pd.DataFrame(lstRelevantDistGroup, columns=[\"Section\", \"Document\", \"Requirement\", \"Content\", \"Distance\"])\n",
    "\n",
    "dfRelevantDist.to_excel(\"./classified_requirements.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Check if the requirements exist in the database\n",
    "\n",
    "In this step, we check if the text that we identified so far is covered by the requirements that exist in the database. We use the sentence transformers to get the embeddings of the text and then we compare them to the existin sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholdPartial = 0.15\n",
    "thresholdCovered = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the requirements from the Excel file\n",
    "dfRelevantSections = pd.read_excel(\"./classified_requirements.xlsx\")\n",
    "\n",
    "lstRelevantSections = dfRelevantSections.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstRelevantSections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentenceTransformer(\"sentence-t5-large\")\n",
    "# model = SentenceTransformer(\"sentence-transformers/gtr-t5-xxl\")\n",
    "\n",
    "# list with all embeddings for the sections\n",
    "lstRelevantEmbeddings = []\n",
    "iCounter = 0\n",
    "\n",
    "for oneLine in tqdm(lstRelevantSections):\n",
    "\n",
    "    # the content of the section starts on the third position of the list\n",
    "    sentences = oneLine[3]\n",
    "\n",
    "    # Sentences are encoded by calling model.encode()\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Print the average embeddings for all the sentences \n",
    "    # in this section\n",
    "    # avg_embedding = np.mean(embeddings, axis=0)\n",
    "    \n",
    "    lstOneLine = [oneLine[0], oneLine[1], 2, str(sentences).replace(\"$\", \"_\").replace(\"\\n\", \"_\"), embeddings]\n",
    "\n",
    "    lstRelevantEmbeddings.append(lstOneLine)\n",
    "\n",
    "    iCounter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, read the requirements from the requirements database, file 20_requirements.xlsx\n",
    "dfTRequirements = pd.read_excel(\"existing_requirements.xlsx\")\n",
    "\n",
    "# convert to list\n",
    "lstTRequirements = dfTRequirements.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstTRequirements[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the embeddings\n",
    "# model = SentenceTransformer(\"sentence-t5-large\")\n",
    "\n",
    "# list with all embeddings for the sections\n",
    "lstTRequirementsEmbeddings = []\n",
    "iCounter = 0\n",
    "\n",
    "for oneLine in tqdm(lstTRequirements):\n",
    "\n",
    "    # the content of the section starts on the requirement text\n",
    "    sentences = oneLine[1]\n",
    "\n",
    "    # Sentences are encoded by calling model.encode()\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Print the average embeddings for all the sentences \n",
    "    # in this section\n",
    "    #avg_embedding = np.mean(embeddings, axis=0)\n",
    "    \n",
    "    lstOneLine = [oneLine[0], oneLine[1], 2, str(sentences).replace(\"$\", \"_\").replace(\"\\n\", \"_\"), embeddings]\n",
    "\n",
    "    lstTRequirementsEmbeddings.append(lstOneLine)\n",
    "\n",
    "    iCounter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the embeddings for all the lstTRequirementsEmbeddings\n",
    "lstTRequirementsEmbeddingsNP = np.array([x[4] for x in lstTRequirementsEmbeddings])\n",
    "\n",
    "tRequirementsAvgEmbeddings = np.mean(lstTRequirementsEmbeddingsNP, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, calculate the average distance of all the relevant sections to the average requirements\n",
    "lstDist = []\n",
    "\n",
    "for oneLine in tqdm(lstRelevantEmbeddings):\n",
    "    # euclidean distance between the two embeddings\n",
    "    dist = distance.cosine(oneLine[4], tRequirementsAvgEmbeddings)\n",
    "    lstDist.append([oneLine[0], oneLine[1], dist, oneLine[3]])\n",
    "\n",
    "# now we sort the list by the distance\n",
    "lstDist.sort(key=lambda x: x[3])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bar plot for the distances lstDist[2]\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "# Create a bar plot\n",
    "plt.bar([x[0] for x in lstDist], [x[2] for x in lstDist])\n",
    "\n",
    "# Add labels to each dot\n",
    "for i, label in enumerate([x[1] for x in lstDist]):\n",
    "    plt.text(i, lstDist[i][2], label)\n",
    "\n",
    "# add horizontal lines for the thresholds\n",
    "plt.axhline(y=thresholdPartial, color='orange', linestyle='-')\n",
    "plt.axhline(y=thresholdCovered, color='red', linestyle='-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results using the scatter plot, where\n",
    "# the x-axis is the distance to the average requirement\n",
    "# the y-axis is always 1\n",
    "# the color is red for the requirements\n",
    "\n",
    "# Create a color map based on x[2]\n",
    "color_map = {0: 'blue', 1: 'green', 2: 'red'}\n",
    "colors = [color_map[0] for x in lstDist]\n",
    "\n",
    "# we plot the t-SNE results\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "plt.scatter([x[2] for x in lstDist], [random.random() * 2 for x in lstDist], c=colors, s=100, alpha=0.5)\n",
    "\n",
    "# Add labels to each dot\n",
    "#for i, label in enumerate([x[3] for x in lstDist]):\n",
    "#    plt.text([x[2] for x in lstDist][i], 1, label[:20]+'...', rotation='vertical')\n",
    "\n",
    "# Add vertical lines at 0.33 and 1.0\n",
    "plt.axvline(x=thresholdPartial, color='orange', linestyle='--')\n",
    "plt.axvline(x=thresholdCovered, color='red', linestyle='--')\n",
    "\n",
    "# Set the limits of the x-axis\n",
    "plt.xlim(0, 2)\n",
    "plt.ylim(0, 2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we separate them into three lists:\n",
    "# - with distance below 0.33 -- these requirements are covered\n",
    "# - with distance between 0.33 and 1.0 -- these requirements are partially covered\n",
    "# - with distance above 1.0 -- these requirements are not covered\n",
    "\n",
    "lstCovered = []\n",
    "lstPartiallyCovered = []\n",
    "lstNotCovered = []\n",
    "\n",
    "for oneLine in lstDist:\n",
    "    if oneLine[2] < thresholdPartial:\n",
    "        lstCovered.append(oneLine)\n",
    "    elif oneLine[2] < thresholdCovered:\n",
    "        lstPartiallyCovered.append(oneLine)\n",
    "    else:\n",
    "        lstNotCovered.append(oneLine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: create requirements based on each of these sections\n",
    "\n",
    "In the last step, we create new requirements based on the sections identified in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0)\n",
    "\n",
    "modelInstr = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\", \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "tokenizerInstr = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRequirement(content, type, model, tokenizer):\n",
    "    content1 = content.split(\",\")\n",
    "    content1 = [x for x in content1[1:] if x not in ['', \" ''\", \" '']\"]]\n",
    "    content_str = \" \".join(content1)\n",
    "    \n",
    "    strContent = f\"Based on this : {content_str}. Write the requirement in the following format 'The system shall ' \"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": strContent},\n",
    "    ]\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "\n",
    "    output = pipe(messages, **generation_args)\n",
    "    \n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstGenerated = []\n",
    "iCounter = 0\n",
    "\n",
    "# we generate new requirements for the partially covered ones\n",
    "if len(lstPartiallyCovered) > 0:\n",
    "    for oneLine in tqdm(lstPartiallyCovered):\n",
    "        if (len(oneLine[3]) < 4095):\n",
    "            strRequirement = createRequirement(oneLine[3], oneLine[2], modelInstr, tokenizerInstr)\n",
    "            lstGenerated.append([oneLine[0], oneLine[1], oneLine[2], oneLine[3], strRequirement])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOutput = pd.DataFrame(lstGenerated, columns=[\"Section\", \"Document\", \"Content\", \"Distance\", \"Generated requirement\"])\n",
    "dfOutput.to_excel(\"./output_partially_covered_generated.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstGenerated = []\n",
    "iCounter = 0\n",
    "\n",
    "# we generate new requirements for the not covered requirements\n",
    "if len(lstNotCovered) > 0:\n",
    "    for oneLine in tqdm(lstNotCovered):\n",
    "        iCounter += 1\n",
    "        if (len(oneLine[3]) < 10000):\n",
    "            strRequirement = createRequirement(oneLine[3], oneLine[2], modelInstr, tokenizerInstr)\n",
    "            lstGenerated.append([oneLine[0], oneLine[1], oneLine[2], oneLine[3], strRequirement])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOutput = pd.DataFrame(lstGenerated, columns=[\"Section\", \"Document\", \"Content\", \"Distance\", \"Generated requirements\"])\n",
    "dfOutput.to_excel(\"./output_not_covered_generated.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstGenerated = []\n",
    "iCounter = 0\n",
    "\n",
    "# we generate new requirements for the not covered requirements\n",
    "if len(lstCovered) > 0:\n",
    "    for oneLine in tqdm(lstCovered):\n",
    "        iCounter += 1\n",
    "        if (len(oneLine[3]) < 10000):\n",
    "            strRequirement = createRequirement(oneLine[3], oneLine[2], modelInstr, tokenizerInstr)\n",
    "            lstGenerated.append([oneLine[0], oneLine[1], oneLine[2], oneLine[3], strRequirement])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOutput = pd.DataFrame(lstGenerated, columns=[\"Section\", \"Document\", \"Content\", \"Distance\", \"Generated requirements\"])\n",
    "dfOutput.to_excel(\"./output_covered_generated.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
